{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88403b2-03b4-45ea-a38c-2cad198bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd7cea-efd9-4d35-a7fc-c8f90a3a7b42",
   "metadata": {},
   "source": [
    "## Building for the data module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061422c-8432-4d1d-8b1e-dc16494f71e6",
   "metadata": {},
   "source": [
    "## Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43cec7-83b5-4b90-ace0-9df12971ca62",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db627d25-73fe-417b-baee-92ca52d18d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp.text.funcutils import forEach, filterBy, yield_forEach, apply, calls, rules\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "@forEach(lowercase)  # performs the reformatting for each line\n",
    "@filterBy(shu_file_line_rule)\n",
    "@filterBy(helsinki_shufile_line_rule)\n",
    "def load_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "@yield_forEach(lowercase)  # performs the reformatting for each line\n",
    "@filterBy(rules(shu_file_line_rule, helsinki_shufile_line_rule))\n",
    "def gen_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        for line in rb:\n",
    "            yield line\n",
    "\n",
    "@apply(calls(lowercase, remove_punctuations))\n",
    "def load_text_from_file(file_path: Union[str, os.PathLike]) -> str:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.read()\n",
    "\n",
    "# load_lines_from_file(sample_file)\n",
    "# load_text_from_file(sample_file)\n",
    "    \n",
    "# for i in gen_lines_from_file(sample_file):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2296d2-9183-4d36-8feb-ecb2bd8f0576",
   "metadata": {},
   "source": [
    "## Flow the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d6ad6d8-4482-44d0-9653-0fb22989fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections.abc import Callable\n",
    "from typing import Union\n",
    "\n",
    "from marynlp.text.data.objects import mask_token, token\n",
    "from marynlp.text.processors.formatters import white_space_cleaning\n",
    "\n",
    "def replace_text_to_token(input_: Union[str, token, mask_token], is_text: Callable[str], m_token: mask_token) -> Union[token, mask_token]:    \n",
    "    # Check if the input is test\n",
    "    if isinstance(input_, str):\n",
    "        if is_text(input_):\n",
    "            return m_token\n",
    "\n",
    "    return input_\n",
    "# str_.upper()\n",
    "# replace_number(\"I have 3,000,000 million dollars\") # fails\n",
    "# replace_number(\"The distance is 23.45 kilometers\") # fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdcbaf2b-8856-434a-bdb8-78e302666a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections.abc import Callable\n",
    "from functools import partial\n",
    "from marynlp.text.data.objects import sentence\n",
    "from marynlp.text.funcutils import calls\n",
    "\n",
    "# regex_for_numbers = r'(?<!\\S)(?=.)(0|([1-9](\\d*|\\d{0,2}(,\\d{3})*)))?(\\.\\d*[1-9])?(?!\\S)|(\\d+)'\n",
    "regex_for_numbers = r'(\\d+)'\n",
    "\n",
    "common_flags =  re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "number_re_ = re.compile(regex_for_numbers, common_flags)\n",
    "\n",
    "def is_number(input_: str): return number_re_.match(input_) is not None\n",
    "def mask_number_in_text(text: str):\n",
    "    text_matched = number_re_.split(text)\n",
    "    tokens = map(partial(replace_text_to_token, is_text=is_number, m_token=Vocab.NUM_TOKEN), text_matched)\n",
    "    return tokens\n",
    "\n",
    "# tokens = mask_number_in_text(\"my name is kevin. I am 23 years old\");tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55b31d9-4459-491c-96d4-49bca9b3e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x7fb3f451a7d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Union\n",
    "from marynlp.text.data.objects import mask_token, word\n",
    "from marynlp.text.funcutils import calls, forEach, filterBy\n",
    "\n",
    "base_characters = 'abcdefghijklmnoprstuvwyz'\n",
    "base_numbers = '0123456789'\n",
    "base_word_non_letter_chars = '\\'-'\n",
    "\n",
    "r_sw_word = r'([{}{}{}{}]+)'.format(base_characters, base_characters.upper(), base_numbers, base_word_non_letter_chars)\n",
    "word_re_ = re.compile(r_sw_word, common_flags)\n",
    "\n",
    "def is_swahili_word(input_: str): return word_re_.match(input_) is not None\n",
    "def is_mask_token(input_: Union[Any, mask_token]) -> bool: return isinstance(input_, mask_token)\n",
    "def tokenize_swahili_word(input_: Union[str, token]):\n",
    "    if is_swahili_word(str(input_)):\n",
    "        return token(input_)\n",
    "    \n",
    "    return input_\n",
    "    \n",
    "# def valid_text_rule(text: str) -> bool:\n",
    "#     return len(text.strip()) > 0\n",
    "\n",
    "    \n",
    "# @forEach(tokenize_swahili_word, skip_rule=is_mask_token)\n",
    "# @forEach(calls(mask_number_in_text), skip_rule=is_mask_token)\n",
    "@filterBy(lambda text: len(text.strip()) > 0)  # only work with text that aren't empty | similar to `valid_text_rule`\n",
    "def word_tokenize(text: str):\n",
    "    return word_re_.split(text)\n",
    "\n",
    "word_tokenize(\"my name is kevin. I am 23 years old\") # [t'my', t'name', t'is', t'kevin', '. I ', t'am', <num>, t'years', t'old']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd3d93e-fc64-464f-99e6-38ced0ea240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @forEach(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f626da-c1f7-42da-a293-b5d7da925a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x7fb3f450ba90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
    "word_tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b46ce2-7dc9-4e72-a99f-b5c6e31c06d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ac bibendum augue. Fusce at nisi tortor. Morbi non l',\n",
       " 'igula eu arcu hendrerit viverra eget non elit. Duis blandit ut lorem sit amet vulputate. Sed dignissim justo erat, vel p',\n",
       " 'osuere eros gravida eu. Sed auctor interdum gravida. Maecenas imperdiet at ante in placerat. Quisque faucibus blandit cu',\n",
       " 'rsus. Praesent aliquam tempor magna. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia cu',\n",
       " 'rae; Aenean lacinia lobortis facilisis.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur ac bibendum augue. Fusce at nisi tortor. Morbi non ligula eu arcu hendrerit viverra eget non elit. Duis blandit ut lorem sit amet vulputate. Sed dignissim justo erat, vel posuere eros gravida eu. Sed auctor interdum gravida. Maecenas imperdiet at ante in placerat. Quisque faucibus blandit cursus. Praesent aliquam tempor magna. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean lacinia lobortis facilisis.\n",
    "\"\"\"\n",
    "\n",
    "break_text_to_sentences(text_.strip(), max_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d421d-d1f6-476d-9c8d-94a7cb529ed9",
   "metadata": {},
   "source": [
    "## Figuring out tokenization\n",
    "\n",
    "Identifying how to tokenize a sentence. Doing so such that:\n",
    "\n",
    "Original sentnece:\n",
    "```\n",
    "mwanafunzi anaenda shule\n",
    "```\n",
    "\n",
    "*Word level masking:*\n",
    "```\n",
    "mwanafunzi anaenda [MASK]\n",
    "```\n",
    "`[MASK]` - Masking that happens here is for 2 token (subword): `shule` -> `shu`, `le`\n",
    "\n",
    "*Subword level masking:*\n",
    "```\n",
    "mwanafunzu ana<MASK> shule\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e9ce404-1f9e-4a26-933c-2970ae6bb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Iterable\n",
    "from marynlp.text.data.objects import mask_token, token\n",
    "\n",
    "# objects\n",
    "# ------------------------------------------\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "    This should represent the different information about the Vocabulary\n",
    "    \"\"\"\n",
    "    UNK_TOKEN = mask_token('unk')\n",
    "    NUM_TOKEN = mask_token('num')\n",
    "    \n",
    "    def __init__(self, list_tokens: List[token]):\n",
    "        self.tokens = list_tokens\n",
    "    \n",
    "    def has(self, token_: Union[str, token]) -> bool:\n",
    "        \"\"\"Checks if the vocab object has the token\"\"\"\n",
    "        if isinstance(token_, str):\n",
    "            token_ = token(token_)\n",
    "            \n",
    "        for tok in self.tokens:\n",
    "            if tok.get() == token_.get():\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return list(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_list(cls, list_str: List[str]):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file_path: Union[str, os.PathLike]):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # print the values\n",
    "        if len(self) > 5:\n",
    "            t = self.get_tokens()\n",
    "            return \"{}, ..., {}\".format(\", \".join(t[:2]), t[-1])\n",
    "        \n",
    "        return \", \".join(t)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Vocab(%s, count=%d)' % (self.extra_repr(), len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c7380be-e404-4b08-a19f-929eec912d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mapper([('a', 0), ('d', 1), ('e', 2), ('n', 3), ('r', 6)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict, OrderedDict\n",
    "from typing import List, Tuple, Any, Union\n",
    "\n",
    "class Mapper(object):\n",
    "    def __init__(self, od: OrderedDict):\n",
    "        self._od = od\n",
    "        \n",
    "    @property\n",
    "    def ordered_dict():\n",
    "        \"\"\"Get the ordered dict\"\"\"\n",
    "        return self._od\n",
    "    \n",
    "    def map_(self, key: str):\n",
    "        if key not in self._od:\n",
    "            raise KeyError(\"Mapping key '%s' doesn't exist in the Mapper\" % key)\n",
    "\n",
    "        return self._od[key]\n",
    "    \n",
    "    def add(self, key: str, value: Any):\n",
    "        assert key not in self._od, \"Mapping key '%s' already exists\" % key\n",
    "        self._od[key] = value\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_list_tuple(cls, list_o_tuple: Union[zip, List[Tuple[str, Any]]]):\n",
    "        return cls(OrderedDict(list_o_tuple))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, dict_: Dict[str, Any]):\n",
    "        return cls(OrderedDict(dict_))\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return list(self._od.items())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Mapper({})\".format(self.extra_repr())\n",
    "\n",
    "class Encoder(Mapper):    \n",
    "    @classmethod\n",
    "    def from_decoder(cls, decoder: Decoder):\n",
    "        return Encoder(decoder.ordered_dict)\n",
    "\n",
    "class Decoder(Mapper):\n",
    "    @classmethod\n",
    "    def from_encoder(cls, encoder: Encoder):\n",
    "        return Decoder(encoder.ordered_dict)\n",
    "        pass\n",
    "\n",
    "items = sorted(list(set('anaenda'))); items\n",
    "mapper = Mapper.from_list_tuple(zip(items, range(len(items))))\n",
    "mapper.add('r', 6)\n",
    "\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e800b-c181-4633-b37b-2858506717a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33392516-befd-42eb-8789-017ef0afb687",
   "metadata": {},
   "source": [
    "## Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6aba72-43b6-4670-8c95-4572eba14825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectors: filters\n",
    "# -----------------------\n",
    "\n",
    "def shu_file_line_rule(text: str) -> bool:\n",
    "\n",
    "    # Check if there is a text that has <text\n",
    "    if text.find(\"<text\") >= 0: return False\n",
    "\n",
    "    # Check if there is a text that has </text>\n",
    "    if text.find(\"</text>\") >= 0: return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Making selection of data\n",
    "def content_width_line_rule(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Selectors to choose the lines that work for downstream processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # if line is less than 20, done select for processing\n",
    "    if len(text) < 20: return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def break_text_to_sentences(text: str, max_length: int = 120):\n",
    "    assert isinstance(text, str), \"text should be string\"\n",
    "    _l = len(text)\n",
    "    sentences = []\n",
    "    \n",
    "    ix, end = 0, 0\n",
    "    for i in range(_l):\n",
    "        if (i + 1) % max_length == 0:\n",
    "            ix, end = end, i\n",
    "            sentences.append(text[ix:end])\n",
    "\n",
    "    # pass last sentence\n",
    "    ix, end = end, _l\n",
    "    sentences.append(text[ix:end])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f036a862-5b52-4fe6-8a96-4e6935ec333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp.text import funcutils as f\n",
    "\n",
    "@f.flowBy(break_text_to_sentences)\n",
    "@f.forEach(lowercase)  # performs the reformatting for each line\n",
    "@f.filterBy(f.rules(shu_file_line_rule, content_width_line_rule))\n",
    "def load_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbefd44-fb3a-46f9-adcb-b95dc1dd9933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object load_lines_from_file at 0x7ffb74222450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "load_lines_from_file(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c9646-bd12-4b3b-a592-2cb81da7fbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec5df9-b51e-4bae-81f0-710253dbfceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "pythonjvsc74a57bd069a7e0843bc35a71a97378535d2c070d3d2cfd63b216ff99a2c8e0e8d31df048"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

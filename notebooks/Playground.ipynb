{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88403b2-03b4-45ea-a38c-2cad198bebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1261f550-963f-4081-95c6-ed511b04f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def ignore_rules(text: str):\n",
    "    return not (text.find(\"<text\") > -1 or text.find(\"</text>\") > -1)\n",
    "\n",
    "def should_be_longer_that_20(text: str):\n",
    "    return len(text) > 20\n",
    "\n",
    "@f.forEach(lowercase)\n",
    "@f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))\n",
    "def load_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.readlines()\n",
    "    \n",
    "filtered_fn = f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))(load_file)\n",
    "# filtered_fn(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cc6a5-8ce6-4b86-be2d-a85f4360c9e1",
   "metadata": {},
   "source": [
    "# KaDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c01e5c3-e537-4ab5-bc0e-8156c2bdac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecd0f13-bacc-4a7d-a8d5-b22cbebe75dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def not_have_html(text: str) -> bool:\n",
    "    return not text.find('<text') > -1\n",
    "\n",
    "\n",
    "def read_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.readlines()\n",
    "    \n",
    "# read_file(sample_file)\n",
    "not_have_html(\"<text filename=\\\"Helsinki Corpus of Swahili/new-mat/bunge/han1-2004.shu\\\" title=\\\"Majadiliano ya Bunge\\\" year=\\\"200\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd7cea-efd9-4d35-a7fc-c8f90a3a7b42",
   "metadata": {},
   "source": [
    "## Building for the data module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061422c-8432-4d1d-8b1e-dc16494f71e6",
   "metadata": {},
   "source": [
    "## Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43cec7-83b5-4b90-ace0-9df12971ca62",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db627d25-73fe-417b-baee-92ca52d18d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp.funcutils import forEach, filterBy, yield_forEach, apply, calls, rules\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "@forEach(lowercase)  # performs the reformatting for each line\n",
    "# @filterBy(shu_file_line_rule)\n",
    "# @filterBy(helsinki_shufile_line_rule)\n",
    "def load_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "@yield_forEach(lowercase)  # performs the reformatting for each line\n",
    "# @filterBy(rules(shu_file_line_rule, helsinki_shufile_line_rule))\n",
    "def gen_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        for line in rb:\n",
    "            yield line\n",
    "\n",
    "@apply(calls(lowercase, remove_punctuations))\n",
    "def load_text_from_file(file_path: Union[str, os.PathLike]) -> str:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.read()\n",
    "\n",
    "# load_lines_from_file(sample_file)\n",
    "# load_text_from_file(sample_file)\n",
    "    \n",
    "# for i in gen_lines_from_file(sample_file):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2296d2-9183-4d36-8feb-ecb2bd8f0576",
   "metadata": {},
   "source": [
    "## Flow the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6ad6d8-4482-44d0-9653-0fb22989fb19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'marynlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-174aab3b5b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarynlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplace_text_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'marynlp'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections.abc import Callable\n",
    "from typing import Union\n",
    "\n",
    "from marynlp.text.data.objects import mask_token, token\n",
    "\n",
    "def replace_text_to_token(input_: Union[str, token, mask_token], is_text: Callable[str], m_token: mask_token) -> Union[token, mask_token]:    \n",
    "    # Check if the input is test\n",
    "    if isinstance(input_, str):\n",
    "        if is_text(input_):\n",
    "            return m_token\n",
    "\n",
    "    return input_\n",
    "# str_.upper()\n",
    "# replace_number(\"I have 3,000,000 million dollars\") # fails\n",
    "# replace_number(\"The distance is 23.45 kilometers\") # fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d421d-d1f6-476d-9c8d-94a7cb529ed9",
   "metadata": {},
   "source": [
    "## Figuring out tokenization\n",
    "\n",
    "Identifying how to tokenize a sentence. Doing so such that:\n",
    "\n",
    "Original sentnece:\n",
    "```\n",
    "mwanafunzi anaenda shule\n",
    "```\n",
    "\n",
    "*Word level masking:*\n",
    "```\n",
    "mwanafunzi anaenda [MASK]\n",
    "```\n",
    "`[MASK]` - Masking that happens here is for 2 token (subword): `shule` -> `shu`, `le`\n",
    "\n",
    "*Subword level masking:*\n",
    "```\n",
    "mwanafunzu ana<MASK> shule\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9ce404-1f9e-4a26-933c-2970ae6bb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Iterable, Optional\n",
    "from marynlp.text.data.objects import mask_token, token\n",
    "\n",
    "# objects\n",
    "# ------------------------------------------\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "    This should represent the different information about the Vocabulary\n",
    "    \"\"\"\n",
    "    UNK_TOKEN = mask_token('unk')\n",
    "    NUM_TOKEN = mask_token('num')\n",
    "    \n",
    "    def __init__(self, list_tokens: Optional[List[token]] = None):\n",
    "        if list_tokens is None:\n",
    "            list_tokens = []\n",
    "\n",
    "        self.tokens = list_tokens\n",
    "        \n",
    "    def add_token_list(self, token_list: List[token]):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def has(self, token_: Union[str, token]) -> bool:\n",
    "        \"\"\"Checks if the vocab object has the token\"\"\"\n",
    "        if isinstance(token_, str):\n",
    "            token_ = token(token_)\n",
    "            \n",
    "        for tok in self.tokens:\n",
    "            if tok.get() == token_.get():\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return list(self.tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_list(cls, list_str: List[str]):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file_path: Union[str, os.PathLike]):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # print the values\n",
    "        if len(self) > 5:\n",
    "            t = self.get_tokens()\n",
    "            return \"{}, ..., {}\".format(\", \".join(t[:2]), t[-1])\n",
    "        \n",
    "        return \", \".join(t)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Vocab(%s, count=%d)' % (self.extra_repr(), len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7380be-e404-4b08-a19f-929eec912d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mapper([('a', 0), ('d', 1), ('e', 2), ('n', 3), ('r', 6)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict, OrderedDict\n",
    "from typing import List, Tuple, Any, Union\n",
    "\n",
    "class Mapper(object):\n",
    "    def __init__(self, od: OrderedDict):\n",
    "        self._od = od\n",
    "        \n",
    "    @property\n",
    "    def ordered_dict():\n",
    "        \"\"\"Get the ordered dict\"\"\"\n",
    "        return self._od\n",
    "    \n",
    "    def map_(self, key: str):\n",
    "        if key not in self._od:\n",
    "            raise KeyError(\"Mapping key '%s' doesn't exist in the Mapper\" % key)\n",
    "\n",
    "        return self._od[key]\n",
    "    \n",
    "    def add(self, key: str, value: Any):\n",
    "        assert key not in self._od, \"Mapping key '%s' already exists\" % key\n",
    "        self._od[key] = value\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_list_tuple(cls, list_o_tuple: Union[zip, List[Tuple[str, Any]]]):\n",
    "        return cls(OrderedDict(list_o_tuple))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, dict_: Dict[str, Any]):\n",
    "        return cls(OrderedDict(dict_))\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return list(self._od.items())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Mapper({})\".format(self.extra_repr())\n",
    "\n",
    "class Encoder(Mapper):    \n",
    "    @classmethod\n",
    "    def from_decoder(cls, decoder: Decoder):\n",
    "        return Encoder(decoder.ordered_dict)\n",
    "\n",
    "class Decoder(Mapper):\n",
    "    @classmethod\n",
    "    def from_encoder(cls, encoder: Encoder):\n",
    "        return Decoder(encoder.ordered_dict)\n",
    "        pass\n",
    "\n",
    "items = sorted(list(set('anaenda'))); items\n",
    "mapper = Mapper.from_list_tuple(zip(items, range(len(items))))\n",
    "mapper.add('r', 6)\n",
    "\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06e800b-c181-4633-b37b-2858506717a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33392516-befd-42eb-8789-017ef0afb687",
   "metadata": {},
   "source": [
    "## Building pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f75967-f4de-4ff4-8cdc-21e8a6b0acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.data.objects import mask_token, token\n",
    "\n",
    "def tokenize_input(input_: str, apply_rule: Callable) -> token:\n",
    "    if apply_rule(input_):\n",
    "        return token(input_)\n",
    "    \n",
    "    return input_\n",
    "\n",
    "def mask_input(input_: str, apply_rule: Callable, mt_: mask_token) -> mask_token:\n",
    "    if apply_rule(input_):\n",
    "        return mt_\n",
    "    \n",
    "    return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cd6b-cd2c-48a9-bfe9-fb6fcfed85e5",
   "metadata": {},
   "source": [
    "## Regular expresison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74f69006-548a-4026-8206-f4a28c8f91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.data.objects import sentence, mask_token, word, token\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "from typing import Any, Union\n",
    "from functools import partial\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import re\n",
    "\n",
    "# Regular Expression\n",
    "common_flags =  re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "\n",
    "# ------------------------------------\n",
    "# CHECKING FOR NUMBER\n",
    "# ------------------------------------\n",
    "\n",
    "# regex_for_numbers = r'(?<!\\S)(?=.)(0|([1-9](\\d*|\\d{0,2}(,\\d{3})*)))?(\\.\\d*[1-9])?(?!\\S)|(\\d+)'\n",
    "number_re_ = re.compile(r'(\\d+)', common_flags)\n",
    "\n",
    "def is_number(input_: str): return number_re_.match(input_) is not None\n",
    "\n",
    "# ------------------------------------\n",
    "# CHECKING FOR SWAHILI WORD\n",
    "# ------------------------------------\n",
    "\n",
    "base_characters = 'abcdefghijklmnoprstuvwyz'\n",
    "base_numbers = '0123456789'\n",
    "base_word_non_letter_chars = '\\'-'\n",
    "\n",
    "r_sw_word = r'([{}{}{}{}]+)'.format(base_characters, base_characters.upper(), base_numbers, base_word_non_letter_chars)\n",
    "word_re_ = re.compile(r_sw_word, common_flags)\n",
    "\n",
    "def is_swahili_word(input_: str): return word_re_.match(input_) is not None\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# CHECKING FOR PUNCTUATION\n",
    "# ------------------------------------\n",
    "punct_re_ = re.compile(r\"\\W+\", common_flags)\n",
    "def is_punctuation(input_: str): return punct_re_.match(input_) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49db0bda-a94f-4386-922b-06c1b8a07b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ebe20fe7cde7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# @f.forEach(partial(mask_input, apply_rule=(lambda w: not is_swahili_word(w)), mt_=Vocab.UNK_TOKEN), skip_rule=is_ignore_rule)  # mask invalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforEach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# mask numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforEach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhite_space_cleaning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocab' is not defined"
     ]
    }
   ],
   "source": [
    "from marynlp.text.processors.formatters import white_space_cleaning\n",
    "\n",
    "def is_mask_token(input_: Union[Any, mask_token]) -> bool: return isinstance(input_, mask_token)\n",
    "\n",
    "@f.forEach(partial(tokenize_input, apply_rule=is_swahili_word), type_=sentence)\n",
    "@f.filterBy(lambda x: len(x) > 0)\n",
    "@f.forEach(white_space_cleaning)\n",
    "def construct_token_from_text(text: str) -> sentence:\n",
    "    return word_re_.split(text)\n",
    "    \n",
    "\n",
    "is_ignore_rule = f.rules(is_mask_token, is_punctuation)\n",
    "@f.apply(sentence)\n",
    "# @f.forEach(partial(mask_input, apply_rule=(lambda w: not is_swahili_word(w)), mt_=Vocab.UNK_TOKEN), skip_rule=is_ignore_rule)  # mask invalid\n",
    "@f.forEach(partial(mask_input, apply_rule=is_number, mt_=Vocab.NUM_TOKEN))  # mask numbers\n",
    "@f.filterBy(lambda x: len(x) > 0)\n",
    "@f.forEach(white_space_cleaning)\n",
    "def mask_construct_token_from_text(text: str) -> sentence:\n",
    "    return word_re_.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32cd755a-576c-44c4-b7a0-b0d78f5fff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'and'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"This is useful if the `fns` are 'selectors'\"\"\"\u001b[0m   \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mop_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mop_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAND\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_andSelector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mop_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_orSelector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Operation execution required is either `%s` or `%s`\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Projects/ml/packages/mary/marynlp/funcutils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??f.rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d2977-0d64-4c29-b346-7059709146c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_mask_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f108ca46c851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mis_ignore_rule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_mask_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_punctuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mis_ignore_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_mask_token' is not defined"
     ]
    }
   ],
   "source": [
    "is_ignore_rule = f.rules(is_mask_token, is_punctuation, op_=f.Rules.OR)\n",
    "is_ignore_rule(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d21cc41-39b3-4542-89c1-302b3829e1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence('kwa mujibu wa ng'ombe ya 25 na 26 ya mkataba wa stockholm , mkataba huu ni lazima uridhiwe na nchi 50 ndipo utekelezaji wa', l=24)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_token_from_text(\"kwa mujibu wa ng'ombe ya 25 na 26 ya mkataba wa stockholm, mkataba huu ni lazima uridhiwe na nchi 50 ndipo utekelezaji wa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2a23c60-a3c0-414b-b8d5-ed7cfa9fda49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence('kwa mujibu wa ng'ombe ya <num> na <num> ya mkataba wa stockholm , mkataba huu ni lazima uridhiwe na nchi <num> ndipo utekelezaji wa', l=24)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_construct_token_from_text(\"kwa mujibu wa ng'ombe ya 25 na 26 ya mkataba wa stockholm, mkataba huu ni lazima uridhiwe na nchi 50 ndipo utekelezaji wa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13d7b6-6036-408e-883d-ef3ba2b314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250d7ec4-8fdd-4f91-a644-92090986b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "from collections import OrderedDict\n",
    "\n",
    "class FrozenDict(Mapping):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._d = OrderedDict(*args, **kwargs)\n",
    "        self._hash = None # to memoize hash\n",
    "        \n",
    "    def as_dict(self):\n",
    "        return dict(**self._d)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._d)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._d[key]\n",
    "\n",
    "    def __setitem__(self, *args, **kwargs):\n",
    "        raise ValueError(\"Forbidden action. This is a frozen object\")\n",
    "\n",
    "    def __hash__(self):\n",
    "        if self._hash is None:\n",
    "            hash_ = 0\n",
    "            for pair in self.items():\n",
    "                hash_ ^= hash(pair)\n",
    "            self._hash = hash_\n",
    "        return self._hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75d38c-a945-4044-bd75-973defc6cf04",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "This is preferably an immutable object that contains the linguistic information about the data that is being dealt with.\n",
    "\n",
    "This knows information about:\n",
    "- The `token`s that are valid for a language.\n",
    "- The `mask_token`s that are used in the language (if any)\n",
    "- The `compoundToken`s that are used in the language\n",
    "- The `separator` for spliting up words\n",
    "- The rules? to deal with sentences/words/tokens in the lanugage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e742263-37f7-4e20-a0c1-28511736b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\"\"\"\n",
    "According to swahili\n",
    "-------------------\n",
    "\"\"\"\n",
    "from typing import Iterable, Dict, Tuple, Optional, List\n",
    "\n",
    "DEFAULT_SEPARATOR = \"\"\n",
    "\n",
    "def _repr_textarize(str_list: List[str], limit:int=3):\n",
    "    if len(str_list) > limit:\n",
    "        return \"%s, ..., %s\" % (\", \".join(str_list[:limit]), str_list[-1])\n",
    "    \n",
    "    return \", \".join(str_list[:limit])\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    This must be a frozen object\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens: Iterable[str], mask_tokens: Optional[Dict[str, to.mask_token]] = None):\n",
    "        self.separator = DEFAULT_SEPARATOR # This is the separator for the word\n",
    "        self._tk_ls = tuple(tokens)\n",
    "        \n",
    "        self._mt_dict = FrozenDict()\n",
    "        if mask_tokens is not None:\n",
    "            self._mt_dict = FrozenDict(**mask_tokens)\n",
    "            \n",
    "        # get the shape of vocabulary\n",
    "        self.shape = (len(self._tk_ls), len(self._mt_dict))\n",
    "\n",
    "        # For hashing\n",
    "        self._hash = None\n",
    "        \n",
    "        # For representation\n",
    "        self._repr_tokens = None\n",
    "\n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return set(self._tk_ls)\n",
    "    \n",
    "    @property\n",
    "    def masks(self):\n",
    "        return self._mt_dict\n",
    "    \n",
    "    def has(self, token_: str):\n",
    "        return self.has_token(to.token(token_))\n",
    "    \n",
    "    def has_token(self, token: to.token):\n",
    "        return token in self._tk_ls\n",
    "    \n",
    "    def has_mask(self, key: str):\n",
    "        return key in self._mt_dict\n",
    "    \n",
    "    def mask(self, key: str):\n",
    "        try:\n",
    "            return self._mt_dict[key]\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Invalid '{0}' mask identifier. Was the mask='{0}' registered?\".format(key))\n",
    "    \n",
    "    def mutate_tokens(self, tokens: Iterable[str]) -> Vocabulary:\n",
    "        \"\"\".add alternative\"\"\"\n",
    "        token_set = set(self._tk_ls).union(set(map(to.token, tokens)))\n",
    "        mask_token_dict = self._mt_dict.as_dict()\n",
    "        return Vocabulary(token_set, mask_token_dict)\n",
    "    \n",
    "    def mutate_mask(self, mask_tokens: Dict[str, to.mask_token]) -> Vocabulary:\n",
    "        \"\"\".add_mask alternative\"\"\"\n",
    "        token_set = set(self._tk_ls) \n",
    "        mask_token_dict = self._mt_dict.as_dict()\n",
    "        mask_token_dict.update(mask_tokens)\n",
    "        return Vocabulary(token_set, mask_token_dict)\n",
    "    \n",
    "    def save(self, vocab_file_name: str, base_path: str = \"./\"):\n",
    "        \"\"\"Saves the data\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, vocab_file_path: str):\n",
    "        \"\"\"Load the file\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def extra_repr(self) -> Tuple[str, tuple]:\n",
    "        if self._repr_tokens is None:\n",
    "            tokens_ = sorted(self._tk_ls)\n",
    "            masks_ = list(self._mt_dict.as_dict().values())\n",
    "            self._repr_tokens = \"tokens={%s}, masks={%s}\" % (_repr_textarize(tuple(map(str, tokens_))), _repr_textarize(tuple(map(str, masks_))))\n",
    "        \n",
    "        return self._repr_tokens, str(self.shape)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        r_str_, shape = self.extra_repr()\n",
    "        return \"Vocabulary(%s, shape=%s)\" % (r_str_, shape)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        if self._hash is None:\n",
    "            tk = tuple(sorted(self._tk_ls))\n",
    "            mk = tuple(self._mt_dict)\n",
    "            self._hash = hash((tk, mk))\n",
    "            \n",
    "        return self._hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164bb674-3960-4bb4-8dc3-b959fc5abeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.data.objects import sentence, mask_token, word, token\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "from typing import Any, Union\n",
    "from functools import partial\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import re\n",
    "\n",
    "# Regular Expression\n",
    "common_flags =  re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "\n",
    "# ------------------------------------\n",
    "# CHECKING FOR NUMBER\n",
    "# ------------------------------------\n",
    "\n",
    "# regex_for_numbers = r'(?<!\\S)(?=.)(0|([1-9](\\d*|\\d{0,2}(,\\d{3})*)))?(\\.\\d*[1-9])?(?!\\S)|(\\d+)'\n",
    "number_re_ = re.compile(r'(\\d+)', common_flags)\n",
    "\n",
    "def is_number(input_: str): return number_re_.match(input_) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05960b95-479c-4f1f-831a-32e755aa5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "\n",
    "from typing import List\n",
    "from marynlp.text.data import objects as to\n",
    "from marynlp.text.processors import formatters as fmt\n",
    "from marynlp import funcutils as f\n",
    "# selectors: filters\n",
    "# -----------------------\n",
    "\n",
    "def shu_file_line_rule(text: str) -> bool:\n",
    "\n",
    "    # Check if there is a text that has <text\n",
    "    if text.find(\"<text\") >= 0: return False\n",
    "\n",
    "    # Check if there is a text that has </text>\n",
    "    if text.find(\"</text>\") >= 0: return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Making selection of data\n",
    "def content_width_line_rule(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Selectors to choose the lines that work for downstream processing\n",
    "    \"\"\"\n",
    "    \n",
    "    # if line is less than 20, done select for processing\n",
    "    if len(text) < 20: return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def break_text_to_sentences(text: str, max_length: int = 120):\n",
    "    assert isinstance(text, str), \"text should be string\"\n",
    "    _l = len(text)\n",
    "    sentences = []\n",
    "    \n",
    "    ix, end = 0, 0\n",
    "    for i in range(_l):\n",
    "        if (i + 1) % max_length == 0:\n",
    "            ix, end = end, i\n",
    "            sentences.append(text[ix:end])\n",
    "\n",
    "    # pass last sentence\n",
    "    ix, end = end, _l\n",
    "    sentences.append(text[ix:end])\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def is_mask_token(input_: Union[Any, to.mask_token]) -> bool: \n",
    "    \"\"\"Checks if the input is a mask token\"\"\"\n",
    "    return isinstance(input_, to.mask_token)\n",
    "\n",
    "def mask_input(input_: str, apply_rule: Callable, mt_: to.mask_token) -> to.mask_token:\n",
    "    if apply_rule(input_):\n",
    "        return mt_\n",
    "    \n",
    "    return input_\n",
    "\n",
    "@f.forEach(to.token, skip_rule=is_mask_token)\n",
    "@f.forEach(partial(mask_input, apply_rule=is_number, mt_=to.mask_token(\"num\")))\n",
    "def split_by_space(text: str) -> List[Union[to.token, to.mask_token]] :\n",
    "    \"\"\"Breaks a long text into tokens\"\"\"    \n",
    "    # function to clean the text\n",
    "    clean_text_fn = fmt.white_space_cleaning\n",
    "    return re.split(r\"\\s+\", clean_text_fn(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27332dc-c968-45b7-b30a-9e52b9d7c45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[t'lowercase,', t'remove_punctuations,', t'white_space_cleaning', <num>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_by_space(\"lowercase, remove_punctuations, white_space_cleaning 45\")\n",
    "\n",
    "# vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fb0f7-2af6-463d-a01a-01da593f4032",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "This is the took that uses the information in the `ocabulary` to transform the texts accordingly. All transformations are done by the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512e2de3-16af-4f3c-b3fb-a1cad8d20351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Callable\n",
    "from typing import Optional, Iterable, Union\n",
    "\n",
    "from marynlp.text.data import objects as t\n",
    "from marynlp.text.data.objects import sentence, mask_token, word, token\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "from typing import Any, Union\n",
    "from functools import partial, wraps\n",
    "\n",
    "import re\n",
    "\n",
    "# Regular Expression\n",
    "common_flags =  re.UNICODE | re.MULTILINE | re.DOTALL\n",
    "\n",
    "# ------------------------------------\n",
    "# CHECKING FOR NUMBER\n",
    "# ------------------------------------\n",
    "\n",
    "# regex_for_numbers = r'(?<!\\S)(?=.)(0|([1-9](\\d*|\\d{0,2}(,\\d{3})*)))?(\\.\\d*[1-9])?(?!\\S)|(\\d+)'\n",
    "number_re_ = re.compile(r'(\\d+)', common_flags)\n",
    "\n",
    "def is_number(input_: str): return number_re_.match(input_) is not None\n",
    "\n",
    "# processors        \n",
    "def text_tokenize(text: str, formatter_fn: Optional[Callable] = None) -> List[str]:\n",
    "    \"\"\"Breaks a long text into tokens\"\"\"    \n",
    "    # function to clean the text\n",
    "    if formatter_fn is not None:\n",
    "        text = formatter_fn(text)\n",
    "\n",
    "    # import when used\n",
    "    import re\n",
    "    return re.split(r\"\\s+\", text)\n",
    "\n",
    "\n",
    "class Tokenizer(object):\n",
    "    \"\"\"Tokenizer, uses vocab for reference\"\"\"\n",
    "    def __init__(self, vocab: Vocabulary):\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # check that there is mask for unknown words\n",
    "        assert self.vocab.has_mask('unk'), \"Mask for unknown token missing\"\n",
    "            \n",
    "        \n",
    "    @classmethod\n",
    "    def token_for_masking(cls, token_: str) -> Union[Dict[str, to.mask_token], str]:\n",
    "        \"\"\"overridable\"\"\"\n",
    "        if is_number(token_):\n",
    "            return { \"number\": to.mask_token('num') }\n",
    "        \n",
    "        return token_\n",
    "    \n",
    "    def t(self, transform_fn):\n",
    "        \"\"\"overridable\"\"\"\n",
    "        return f.forEach(partial(mask_input, apply_rule=is_number, mt_=self.vocab.mask('number')))(transform_fn)\n",
    "    \n",
    "\n",
    "    def _is_not_in_vocab(self, input_: str) -> bool:\n",
    "        print(input_)\n",
    "        return not self.vocab.has_token(to.token(input_))\n",
    "    \n",
    "    def transform(self, transform_fn: Callable):\n",
    "        @wraps(transform_fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            f_ = self.t(transform_fn)\n",
    "            # final transform if missing\n",
    "            f_ = f.forEach(partial(mask_input, apply_rule=self._is_not_in_vocab, mt_=self.vocab.mask('unk')), skip_rule=is_mask_token)(f_)\n",
    "            \n",
    "            return f_(*args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    # final\n",
    "    @classmethod\n",
    "    def initialize(cls, token_iterable_: Iterable[str], unknown_token: str = 'unk') -> Tokenizer:\n",
    "        w = set()\n",
    "        mk = dict()\n",
    "    \n",
    "        for token_ in token_iterable_:\n",
    "            # check to mask as number\n",
    "            out = cls.token_for_masking(token_)\n",
    "\n",
    "            if isinstance(out, str):\n",
    "                w.add(to.token(token_))\n",
    "            else:\n",
    "                mk.update(out)\n",
    "\n",
    "        vocab = Vocabulary(w, mk).mutate_mask({ \"unk\": to.mask_token(unknown_token) })\n",
    "        return cls(vocab), vocab\n",
    "\n",
    "# or\n",
    "\n",
    "def tokenizer(vocab: Vocabulary, text_splitter: Callable):\n",
    "    def text_tokenize(text: str):\n",
    "        return re.split(r\"\\s+\", text)\n",
    "    return text_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2c41f-7084-4fb5-8edc-67da6413b177",
   "metadata": {},
   "source": [
    "## Defining pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "369aa32f-9374-4a18-bb91-69f656079253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors import formatters as fmt # lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "\n",
    "@f.forEach(f.calls(fmt.lowercase, fmt.white_space_cleaning))  # performs the reformatting for each line\n",
    "@f.filterBy(f.rules(shu_file_line_rule, content_width_line_rule))\n",
    "def load_lines_from_file(file_path: Union[str, os.PathLike]) -> List[str]:\n",
    "    file_path = Path(file_path)\n",
    "    assert file_path.exists(), \"The file doesn't exits\"\n",
    "    \n",
    "    with open(file_path, mode='r', encoding=\"utf8\") as rb:\n",
    "        return rb.readlines()\n",
    "    \n",
    "    # tokenize the texts\n",
    "load_tokens_from_file = f.flowBy(text_tokenize)(load_lines_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dbefd44-fb3a-46f9-adcb-b95dc1dd9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ec5df9-b51e-4bae-81f0-710253dbfceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary(tokens={'kutakuwa, 'mnyamwezi', 'mzigo, ..., zuri.}, masks={<num>, <unk>}, shape=(5419, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marynlp import funcutils as f\n",
    "from marynlp.text.data import objects as to\n",
    "\n",
    "tokenizer, vocab = Tokenizer.initialize(load_tokens_from_file(sample_file))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a540e4-bed3-4c12-b0fd-5d9fb5e7670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdasd\n",
      "kevin\n",
      "mzigo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<unk>, <num>, <num>, <unk>, 'mzigo']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkrz = tokenizer.transform(text_tokenize)\n",
    "tkrz(\"asdasd 4545 32423 kevin mzigo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af016d36-e4f7-48a7-970c-c0129107dcea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a282bf-4f3a-4b3c-90e7-9500da26d0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "pythonjvsc74a57bd069a7e0843bc35a71a97378535d2c070d3d2cfd63b216ff99a2c8e0e8d31df048"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

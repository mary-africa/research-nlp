{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62bcb026-ef22-4f3b-ab45-fb6653082296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3812e208-2b25-465a-bba4-4349be507b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for the SED\n",
    "from marynlp.modules.source import save_model_from_google_bucket\n",
    "from marynlp.utils.storage import download, get_bucket\n",
    "\n",
    "\n",
    "# Download the morphmeme file\n",
    "bucket = get_bucket(\"../resources/mary_africa_credentials_key.json\", \"marynlp-private\")\n",
    "morph_template_file = save_model_from_google_bucket(\"models/sed_morpheme_template.txt\", bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d112c8e-173d-499d-9234-71e6e109b804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/iam-kevin/.marynlp/store/models/sed_morpheme_template.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_template_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5c048d-aa0c-4b21-94db-d72801346029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the words\n",
    "from marynlp import funcutils as f\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Union\n",
    "import os\n",
    "import re\n",
    "\n",
    "def split_by_space(text: str) -> List[str]:\n",
    "    \"\"\"Split a text to word strings\n",
    "    \n",
    "    Example\n",
    "    \"Lorem ipsum\" -> [ 'Lorem', 'ipsum' ] \n",
    "    \"\"\"\n",
    "    return re.split(r\"\\s+\", text)\n",
    "\n",
    "def ignore_rules(text: str) -> bool:\n",
    "    return not (text.find(\"<text\") > -1 or text.find(\"</text>\") > -1)\n",
    "\n",
    "def should_be_longer_than_20(text: str) -> bool:\n",
    "    return len(text) > 20\n",
    "\n",
    "@f.filterBy(f.rules(ignore_rules, should_be_longer_than_20))\n",
    "def read_file(file_path) -> List[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "@f.forEach(str, type_=set)\n",
    "@f.filterBy(lambda s: len(s.strip()) > 0)\n",
    "@f.flowBy(split_by_space)\n",
    "@f.forEach(lambda s: s.strip())\n",
    "def get_unique_words_from_shu_file(file_path: os.PathLike):\n",
    "    return tqdm(read_file(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1b9eb4-5b08-49e3-acb4-a7b08b46ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from experimental.sed import MorphologyAnalyzer\n",
    "from marynlp import funcutils as f \n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "def break_word(word: str, analyzer: MorphologyAnalyzer) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "# Might not want to add this since it's pytorch specific?\n",
    "class MorphemeDataset(Dataset):\n",
    "    def __init__(self, word_iter: Iterable[str], break_word: Callable):\n",
    "        self.break_word = break_word\n",
    "        self.wml = list(word_iter)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.break_word(self.wml[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef631a2-202a-4845-98b4-e35ef031f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f67cc8a-1670-46f1-ad0f-2fc8bcc0d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 763155.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from marynlp.text.data import Vocab, morph\n",
    "from marynlp.text import formatter as fmt\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from marynlp.utils import process as p\n",
    "\n",
    "unique_words = get_unique_words_from_shu_file(\"../resources/data/hcs-na-v2/new-mat/news/alasiri-2009.shu\")\n",
    "analyzer = MorphologyAnalyzer(morph_template_file)\n",
    "\n",
    "# start with remove punctuations\n",
    "clean_text = f.calls(fmt.remove_punctuations, fmt.white_space_cleaning, fmt.lowercase)\n",
    "\n",
    "@f.apply(f.calls(chain.from_iterable, set))\n",
    "def get_unique_morphemes(words: Iterable[str]):\n",
    "    breaker = f.calls(clean_text, f.partial(break_word, analyzer=analyzer))\n",
    "    for w in tqdm(words):\n",
    "        try:\n",
    "            yield breaker(w)\n",
    "        except Exception as e:\n",
    "            print(\"Unable to break word: '%s'\" % w)\n",
    "            raise e\n",
    "\n",
    "            \n",
    "# unique_morphemes = get_unique_morphemes(unique_words)\n",
    "# morpheme_vocab = Vocab(unique_morphemes)\n",
    "# word_vocab = Vocab(map(clean_text, unique_words))\n",
    "# dataset = MorphemeDataset(unique_words, break_word=f.partial(break_word, analyzer=analyzer))\n",
    "# clean_text(\"sisi?,!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff18372-28cf-4e2a-a450-a58edd5a2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  = f.partial(break_word, analyzer=analyzer)\n",
    "# get_unique_morphemes(\"kevin james\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7540a5-8e3c-4f69-8526-1fc0d53f4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This must be updated the data\n",
    "#  in accordance to the data that is used to train the data\n",
    "\n",
    "# word_vocab, morpheme_vocab # Vocab([ , 0..., zwa], len=569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8910cd9-43d9-4553-917f-cc0516fe8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.sed.nn import SEDLanguageModel\n",
    "from experimental.sed.modules.embeddings import SEDWordEmbeddings\n",
    "\n",
    "# swe = SEDWordEmbeddings(\n",
    "#     morpheme_vocab.size,\n",
    "#     embedding_dim=100,\n",
    "#     hidden_dim=32\n",
    "# )\n",
    "\n",
    "def lm_pad_collate(batch):\n",
    "    x, x_len = [], []\n",
    "    \n",
    "    x_len = [m.shape[0] for m in batch]\n",
    "    max_len = max(x_len)\n",
    "    \n",
    "    x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "    xx = torch.stack([torch.from_numpy(x).long() for x in x_batch_])\n",
    "\n",
    "    return xx, torch.as_tensor(x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec098054-2507-4c5f-9b1e-ae75eb11dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from marynlp.text.data import token\n",
    "from marynlp.text.processor import TokenEncoder\n",
    "\n",
    "from marynlp.text import formatter as fmt\n",
    "from itertools import chain\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "def split_text_by_space(text: str) -> Tuple[str]:\n",
    "    return re.split(r'\\s+', fmt.white_space_cleaning(text)) \n",
    "\n",
    "\n",
    "@f.apply(list)\n",
    "def text_pad_sequence(word_sequence: Iterable[Any], padding_length: int, pad_marker: Any):\n",
    "    if len(word_sequence) >= padding_length:\n",
    "        return word_sequence\n",
    "    \n",
    "    return chain.from_iterable((word_sequence, [pad_marker] * (padding_length - len(word_sequence))))\n",
    "\n",
    "@f.apply(tuple)\n",
    "@f.apply(lambda o: zip(*o))\n",
    "def width_pad_sequence(var_sequences: Iterable[List[Any]], padding_length: int, pad_marker: Any):\n",
    "    for var_seq in var_sequences:\n",
    "        out = tuple(var_seq)\n",
    "        yield tuple(chain.from_iterable((out, [pad_marker] * (padding_length - len(var_seq))))), len(out)\n",
    "\n",
    "        \n",
    "# PAD_TOKEN = token('<PAD>')\n",
    "\n",
    "\n",
    "# morph_encoder = TokenEncoder(morpheme_vocab)\n",
    "# word_to_morpheme = f.partial(break_word, analyzer=analyzer)\n",
    "# encode_morpheme = f.apply(np.array)(f.forEach(morph_encoder.encode)(word_to_morpheme))\n",
    "\n",
    "\n",
    "# def pad_collateV2(batch):\n",
    "#     \"\"\"NOTE: This is the modified version from the package\"\"\"\n",
    "#     x, x_len = [], []\n",
    "    \n",
    "#     x_len = [m.shape[0] for m in batch]\n",
    "#     max_len = max(x_len)\n",
    "    \n",
    "#     x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "#     xx = torch.stack([torch.from_numpy(x).long() for x in x_batch_])\n",
    "\n",
    "#     return xx, torch.as_tensor(x_len)\n",
    "\n",
    "\n",
    "\n",
    "# @f.apply(list)\n",
    "def morph_sentence_pad_collate(sentences: List[str], padding_idx: int = 0):\n",
    "    word_sequences = list(map(split_text_by_space, sentences))\n",
    "#     print(word_sequences)\n",
    "    \n",
    "    # padd each sequence\n",
    "    out_list = [list(map(word_to_morpheme, b)) for b in word_sequences]\n",
    "    print(out_list)\n",
    "\n",
    "    # get the longest character sequence in the entire batch\n",
    "    to_pad_morph_length = len(max(list(map(f.partial(max, key=len), out_list)), key=len))\n",
    "    to_pad_word_length = max(map(len, out_list))\n",
    "\n",
    "    morph_padd_function = f.partial(width_pad_sequence, padding_length=to_pad_morph_length, pad_marker=padding_idx)\n",
    "    \n",
    "#         print(\"m_idx_seq\", padd_function(m_idx_seq))\n",
    "    \n",
    "    # hold the number of words and morphme pair count\n",
    "    word_morphemes_count = []\n",
    "    words_tensors = []\n",
    "    words_c = []  # number of words in a sentence\n",
    "\n",
    "    for b in out_list:\n",
    "        word_list = [ list(map(morph_encoder.encode, b_)) for b_ in b ]\n",
    "#         print(word_list)\n",
    "        \n",
    "        padded_morph_sequence, lengths = morph_padd_function(word_list)\n",
    "        \n",
    "#         c, _ = morpheme_pad_collate(list(map(encode_morpheme, b)))\n",
    "#         word_count, longest_token_len = c.shape[0], c.shape[-1]\n",
    "\n",
    "        # grid padding\n",
    "        word_count = len(word_list)\n",
    "        padd_words_count = to_pad_word_length - word_count\n",
    "        out = (tuple([0] * to_pad_morph_length) for _ in range(padd_words_count))    \n",
    "        \n",
    "        words_tensors.append(np.array(list(chain(padded_morph_sequence, out))))\n",
    "#         word_morphemes_count.append(np.array(tuple(chain(lengths, [0] * padd_words_count))))\n",
    "        word_morphemes_count.append(\n",
    "            tuple(chain(\n",
    "                lengths, \n",
    "                [0] * padd_words_count # padding\n",
    "            )))\n",
    "    \n",
    "        words_c.append(word_count)\n",
    "\n",
    "    padded_length_tensors = np.array(word_morphemes_count)\n",
    "#         # get the remaining shape\n",
    "# #         print(c)\n",
    "#     print(out)\n",
    "                             \n",
    "    # padd the sentence objects\n",
    "#     padded_length_tensors = pad_sequence(word_morphemes_count, batch_first=True)\n",
    "\n",
    "    return np.array(words_tensors), np.array(padded_length_tensors), np.array(words_c)\n",
    "\n",
    "# def padded_input(pad_idx: int, padding_length: int):\n",
    "#     return np.array([pad_idx] * padding_length)\n",
    "\n",
    "\n",
    "# t, l = morpheme_pad_collate(list(MorphemeDataset([\"mama\", \"anakuja\"], encode_morpheme))); t\n",
    "\n",
    "# split sentence\n",
    "# lm = DataLoader()\n",
    "# sentences = [\"mama anakuja na mama\", \"mwalimu alikuwa anafundisha\"]\n",
    "# t, mcs, wc = morph_sentence_pad_collate(sentences); \n",
    "# t.shape, mcs.shape, wc.shape\n",
    "\n",
    "# # set_pad = \n",
    "# # print(set_pad)\n",
    "# t[1], mcs[1], wc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8242d7-4dec-4650-ac3b-b3c8e25a35bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<experimental.sed.morphology.MorphologyAnalyzer at 0x7fa50898fd10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e1d1c-2187-46dd-b961-5ec76d2c676e",
   "metadata": {},
   "source": [
    "### Showing Nelson how its done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b362b7eb-e126-4f1c-a30d-3f98a31da334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp import funcutils as f\n",
    "from typing import Tuple\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "def split_text_by_space(text: str) -> Tuple[str]:\n",
    "    return re.split(r'\\s+', fmt.white_space_cleaning(text)) \n",
    "\n",
    "@f.forEach(f.calls(lambda s: s.strip(), split_text_by_space))\n",
    "def get_word_sequences_from_txt_file(file_path: os.PathLike):\n",
    "    return tqdm(read_file(file_path))\n",
    "\n",
    "@f.forEach(str, type_=set)\n",
    "def get_unique_words_from_txt_file(file_path: os.PathLike):\n",
    "    return chain.from_iterable(get_word_sequences_from_txt_file(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55700e98-9b36-4371-b94c-97df4a980bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 93767.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3347/3347 [00:00<00:00, 97418.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29343/29343 [01:18<00:00, 374.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Vocab([UNK, a..., zurich], len=29343), Vocab([a, aa..., zzy], len=5230))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marynlp.text.data import Vocab\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "unique_words = get_unique_words_from_txt_file(\"../resources/nelson/train.txt\") | get_unique_words_from_txt_file(\"../resources/nelson/valid.txt\")\n",
    "# unique_words = set(list(chain.from_iterable(list(map(split_text_by_space, sentences)))))\n",
    "# print(unique_words)\n",
    "unique_morphemes = get_unique_morphemes(unique_words)\n",
    "\n",
    "# print(unique_morphemes)\n",
    "word_vocab = Vocab(unique_words)\n",
    "morpheme_vocab = Vocab(unique_morphemes)\n",
    "\n",
    "\n",
    "word_vocab, morpheme_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48d35c41-8ac1-4871-bca9-2f7c805466f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.processor import PaddedTokenEncoder\n",
    "\n",
    "word_tokenizer = PaddedTokenEncoder(word_vocab)\n",
    "morpheme_tokenizer = PaddedTokenEncoder(morpheme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c7fcbfe-740d-4915-b82c-272f52eae0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345),\n",
       " TokenEncoder(<PAD>=0, <UNK>=1..., zzy=5231, l=5232))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer, morpheme_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d893e78-7b0f-4522-bfff-90063e73ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the embeddings\n",
    "import numpy as np\n",
    "from experimental.sed.modules.embeddings import SEDWordEmbeddings\n",
    "\n",
    "def break_word(word: str, analyzer: MorphologyAnalyzer) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "analyzer = MorphologyAnalyzer(morph_template_file)\n",
    "\n",
    "\n",
    "swe = SEDWordEmbeddings(\n",
    "    morpheme_tokenizer.size,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=32\n",
    ")\n",
    "\n",
    "word_to_morpheme = f.partial(break_word, analyzer=analyzer)\n",
    "encode_morpheme = f.apply(np.array)(f.forEach(morpheme_tokenizer.encode)(word_to_morpheme))\n",
    "morpheme_dataset = MorphemeDataset(word_tokenizer.get_tokens(), break_word=encode_morpheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f38f9c30-1937-4d2b-9095-07928585c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [01:55<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in morpheme_dataset:\n",
    "#     print(i)\n",
    "embeddings = swe.fit(morpheme_dataset)\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40ee86ad-446f-48d2-8815-62e64253b055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29345, 100]),\n",
       " TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, word_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b83f2-c2a7-4d2d-9f4a-09d322db4237",
   "metadata": {},
   "source": [
    "### Finally, the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72261466-7f04-4d0d-aa4b-4852689eb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def next_token_generate(word_sequence: Iterable[Any]) -> Tuple[List[Any], Any]:\n",
    "    word_sequence = tuple(word_sequence)\n",
    "    return word_sequence[:-1], word_sequence[-1]\n",
    "            \n",
    "def next_token_sequence_generator(word_sequence: Iterable[Any]) -> Iterable[Tuple[List[Any], Any]]:\n",
    "    return [next_token_generate(word_sequence)]\n",
    "\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "\n",
    "def flow(iterator_fn):\n",
    "    @wraps(iterator_fn)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return chain.from_iterable(iterator_fn(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "@f.apply(list)\n",
    "@flow\n",
    "def generate_word_next_token_sequence(word_sequences: Iterable[List[str]]):\n",
    "    for w in word_sequences:\n",
    "        yield next_token_sequence_generator(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4aa2857-f754-4fab-9708-e3eea4285ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections.abc import Callable\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, word_sequences: Iterable[Tuple[List[str], str]], word_encode: Callable = None):\n",
    "        self.ls = word_sequences\n",
    "        self.encode = word_encode\n",
    "    \n",
    "    def __getitem__(self, ix: int):\n",
    "        word_seq, next_token = self.ls[ix]\n",
    "        return list(map(self.encode, word_seq)), self.encode(next_token)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8976b63-41b7-487e-8314-3ac6fdc0024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.apply(list)\n",
    "def set_proper_UNK_in_word_sequence(word_sequence):\n",
    "    \"\"\"Fixes the token by replacing UNK with <UNK> (that's a proper unknown token)\"\"\"\n",
    "    for word in word_sequence:\n",
    "        if word == 'UNK':\n",
    "            yield token('<UNK>')\n",
    "            continue\n",
    "        yield token(word)\n",
    "            \n",
    "get_better_sequences_from_txt_file = f.forEach(set_proper_UNK_in_word_sequence)(get_word_sequences_from_txt_file)\n",
    "# word_sequences = get_better_sequences_from_txt_file(\"../resources/nelson/train.txt\")\n",
    "\n",
    "generate_word_sequences_from_file = f.apply(generate_word_next_token_sequence)(get_better_sequences_from_txt_file)\n",
    "build_dataset_from_file = f.apply(f.partial(LMDataset, word_encode=word_tokenizer.encode))(generate_word_sequences_from_file)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4a8b251-1a47-49ff-95e3-aa31f3e535eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 99548.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3347/3347 [00:00<00:00, 90860.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = build_dataset_from_file(\"../resources/nelson/train.txt\") \n",
    "val_dataset = build_dataset_from_file(\"../resources/nelson/valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a18968b-8090-43df-abb6-aa65f755f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_pad_collate(batch: List[\n",
    "    Tuple[ List[int], int ]\n",
    "]):\n",
    "    out, out_len = tuple(zip(*batch))\n",
    "    padded_output = pad_sequence(list(map(torch.tensor, out)), batch_first=True)\n",
    "    return (padded_output, torch.tensor(out_len)) # word_sequence, next_token\n",
    "\n",
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4761ef41-8b52-4e16-9a3f-a73c4dc868f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from time import time\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=lm_pad_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=lm_pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f2184069-9af5-486a-a3c5-212fa3623174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345), 41515)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer, len(word_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ba314383-7a2e-4ca9-8a2b-dc26afab118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, model_name: str, checkpoint_folder_path: str):\n",
    "        \"\"\"Setting the configuration for training the model properly\"\"\"\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        checkpoint_folder_path = Path(checkpoint_folder_path)\n",
    "        assert not Path(checkpoint_folder_path).exists(), \"The folder exists. Delete the folder or enter a new path\"\n",
    "        \n",
    "        # make the folder\n",
    "        checkpoint_folder_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.checkpoint_folder_path = str(checkpoint_folder_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def result_from_checkpoint(cls, model):\n",
    "        \"\"\"Reset from the checking\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save_model_checkpoint(self, info: str):\n",
    "        saved_model_path = str(Path(self.checkpoint_folder_path).joinpath(\"{}.{}.pth\".format(self.model_name, info)))\n",
    "        print(\"Saved! (not really):\", saved_model_path)\n",
    "    \n",
    "    def _train_epoch_step(self, train_dataloader, criterion, optimizer, epoch=1, verbose=100):\n",
    "        start_time = time()\n",
    "        t_batch = len(train_dataloader)\n",
    "        \n",
    "        for batch_idx, _data in enumerate(train_dataloader):\n",
    "            word_sequences, y = _data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(word_sequences)\n",
    "            loss = self.criterion(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % verbose == 0 or batch_idx + 1 == t_batch:\n",
    "                print('Train Epoch: {:03d} [{:03d}/{:03d} ({:.0f}%)]\\tLoss: {:.6f}\\tPerplexity: {:5.2f}\\telapsed: {:.2f} mins'.format(\n",
    "                    epoch, (batch_idx + 1), t_batch,\n",
    "                    100. * ((batch_idx + 1) / t_batch), loss.item(), np.exp(loss.item()), (time() - start_time)/60)\n",
    "                )\n",
    "    \n",
    "    def _val_epoch_step(self, val_dataloader, criterion):\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        self.model.eval()    \n",
    "        test_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for I, _data in enumerate(val_dataloader):\n",
    "                word_sequences, y = _data\n",
    "                output = self.model(word_sequences)\n",
    "                loss = torch.sum(torch.stack([self.criterion(out, y) for out,y in zip([output], y)]))\n",
    "                test_loss += loss.item() / len(val_dataloader)\n",
    "\n",
    "        print('Test set: Average loss: {:.4f} | Perplexity: {:8.2f}\\n'.format(test_loss, np.exp(test_loss)))\n",
    "        return round(test_loss, 4)#, self.model.state_dict()\n",
    "    \n",
    "    def train(self, \n",
    "                train_dataloader, \n",
    "                val_dataloader, \n",
    "                epochs=EPOCHS, \n",
    "                verbose=100,\n",
    "                criterion=F.cross_entropy,\n",
    "                lr=.01\n",
    "             ):\n",
    "        self.model.train()\n",
    "        \n",
    "        optimizer=optim.AdamW(model.parameters(), lr=lr)\n",
    "        accum_loss = torch.tensor(float('inf'))\n",
    "        start_exp_time = time()\n",
    "        \n",
    "        data_len = len(train_dataloader.dataset)\n",
    "        t_batch = len(train_dataloader)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # making the training step\n",
    "            self._train_epoch_step(train_dataloader, criterion, optimizer, epoch=epoch, verbose=verbose)\n",
    "            \n",
    "            test_loss = self._val_epoch_step(val_dataloader, criterion)\n",
    "            \n",
    "            if test_loss < accum_loss:\n",
    "                # Save the model\n",
    "                self.save_model_checkpoint(epoch + \"_loss_{:.02f}\".format(test_loss))\n",
    "                accum_loss = test_loss\n",
    "                stop_eps = 0\n",
    "            else:\n",
    "                stop_eps += 1\n",
    "                \n",
    "            print(\"-\"*60)\n",
    "            epoch_time_elapsed = (time() - start_time)/60\n",
    "            print(\"End of epoch {}/{} | Time elapsed: {} mins | val_loss: {}\".format(epoch, epochs, epoch_time_elapsed, test_loss))\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            # Early stopping\n",
    "            if stop_eps >= early_stop:\n",
    "                # Save the model\n",
    "                self.save_model_checkpoint(epoch + \"_early\")\n",
    "                break\n",
    "\n",
    "    def save_model_checkpoint(self):\n",
    "        best_model, accum_loss = self.model_checkpoints[-1]\n",
    "\n",
    "        if self.save_path is not None and not ModelsConfig.lang_mod:\n",
    "            torch.save(best_model, Path(self.save_path).joinpath(f'{accum_loss}_SAM.pth'))\n",
    "        \n",
    "        if self.save_path is not None and ModelsConfig.lang_mod:\n",
    "            torch.save(best_model, Path(self.save_path).joinpath(f'{accum_loss}_lm.pth'))\n",
    "\n",
    "    def load_model_checkpoint(self, model_weights):\n",
    "        self.model.load_state_dict(torch.load(Path(model_weights)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19b165-36c9-4b47-9429-d66de0f14587",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5024b430-4f3e-45d8-b067-5d8677be2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual training going on\n",
    "!rm -rf ./sed_checkpoint\n",
    "\n",
    "model = SEDLanguageModel(embeddings)\n",
    "trainer = Trainer(model, \"sed_language_model\", \"./sed_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "370aca9e-525c-442f-9e7a-1bb6cd629040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 001 [001/649 (0%)]\tLoss: 10.290546\tPerplexity: 29452.86\telapsed: 0.01 mins\n",
      "Train Epoch: 001 [101/649 (16%)]\tLoss: 6.539432\tPerplexity: 691.89\telapsed: 1.02 mins\n",
      "Train Epoch: 001 [201/649 (31%)]\tLoss: 6.783267\tPerplexity: 882.95\telapsed: 2.13 mins\n",
      "Train Epoch: 001 [301/649 (46%)]\tLoss: 6.689163\tPerplexity: 803.65\telapsed: 3.30 mins\n",
      "Train Epoch: 001 [401/649 (62%)]\tLoss: 7.328592\tPerplexity: 1523.24\telapsed: 4.09 mins\n",
      "Train Epoch: 001 [501/649 (77%)]\tLoss: 6.189862\tPerplexity: 487.78\telapsed: 4.74 mins\n",
      "Train Epoch: 001 [601/649 (93%)]\tLoss: 6.760697\tPerplexity: 863.24\telapsed: 5.60 mins\n",
      "Train Epoch: 001 [649/649 (100%)]\tLoss: 7.102780\tPerplexity: 1215.34\telapsed: 6.14 mins\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15003/843170338.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15003/2037435627.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, val_dataloader, epochs, verbose, criterion, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_epoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maccum_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15003/2037435627.py\u001b[0m in \u001b[0;36m_val_epoch_step\u001b[0;34m(self, val_dataloader, criterion)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mword_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'test_loader'"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, val_dataloader, epochs=10, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "93cf6e6b-d42d-49cd-a30d-7ec632b4f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29345])\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"jumapili\"\n",
    "encoder_sentence = f.forEach(word_tokenizer.encode)(split_by_space)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inp_seq = torch.tensor(encoder_sentence(sample_sentence)).unsqueeze(dim=0)\n",
    "    out = model(inp_seq)\n",
    "    out = F.softmax(out, dim=1)\n",
    "\n",
    "    print(out.shape)    \n",
    "    out = torch.argmax(out)\n",
    "    word = word_tokenizer.decode(out.item())\n",
    "    \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "47055116-14ff-4ce6-9935-09778efe6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 42115.63it/s]\n"
     ]
    }
   ],
   "source": [
    "word_seqs = get_better_sequences_from_txt_file(\"../resources/nelson/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "89d2519d-7e0a-459e-89f4-e049719e58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "all_words_iter = chain.from_iterable(word_seqs)\n",
    "words_occur_last_iter = tuple(word_seq[-1] for word_seq in word_seqs)\n",
    "\n",
    "main_counter = Counter(all_words_iter)\n",
    "last_token_occurance_counter = Counter(words_occur_last_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "42a68ced-5f7f-44ed-bd0c-ff2f038ebf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t|\t%\n",
      "-------------------------\n",
      "<UNK>\t|\t0.136481\n",
      "hiyo\t|\t0.024353\n",
      "alisema\t|\t0.019704\n",
      "hilo\t|\t0.013031\n",
      "huo\t|\t0.012212\n",
      "salaam\t|\t0.010310\n",
      "huu\t|\t0.008358\n",
      "hayo\t|\t0.007780\n"
     ]
    }
   ],
   "source": [
    "total_word_count = sum(map(lambda x: x[1], last_token_occurance_counter.items()))\n",
    "\n",
    "print(\"Token\\t|\\t%\")\n",
    "print(\"-\"*25)\n",
    "for i, c in last_token_occurance_counter.most_common(8):\n",
    "    print(\"{}\\t|\\t{:.06f}\".format(i, c / total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4e15962d-e54e-4048-b673-87011104df1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(t'<UNK>', 5666),\n",
       " (t'hiyo', 1011),\n",
       " (t'alisema', 818),\n",
       " (t'hilo', 541),\n",
       " (t'huo', 507),\n",
       " (t'salaam', 428),\n",
       " (t'huu', 347),\n",
       " (t'hayo', 323),\n",
       " (t'yake', 321),\n",
       " (t'yao', 319),\n",
       " (t'zaidi', 318),\n",
       " (t'wake', 287),\n",
       " (t'nchini', 284),\n",
       " (t'hizo', 281),\n",
       " (t'hicho', 265),\n",
       " (t'hao', 250),\n",
       " (t'zao', 242),\n",
       " (t'moja', 234),\n",
       " (t'tanzania', 229),\n",
       " (t'huyo', 224),\n",
       " (t'wao', 195),\n",
       " (t'serikali', 191),\n",
       " (t'nchi', 179),\n",
       " (t'sasa', 174),\n",
       " (t'taifa', 172),\n",
       " (t'hivyo', 165),\n",
       " (t'tu', 162),\n",
       " (t'sana', 156),\n",
       " (t'mbalimbali', 143),\n",
       " (t'wananchi', 142),\n",
       " (t'kazi', 126),\n",
       " (t'kwanza', 122),\n",
       " (t'jana', 118),\n",
       " (t'mbili', 116),\n",
       " (t'humo', 113),\n",
       " (t'maendeleo', 111),\n",
       " (t'hapo', 108),\n",
       " (t'afrika', 108),\n",
       " (t'ujao', 106),\n",
       " (t'leo', 104),\n",
       " (t'vizuri', 104),\n",
       " (t'anasema', 103),\n",
       " (t'nyingine', 101),\n",
       " (t'yanga', 100),\n",
       " (t'mmoja', 100),\n",
       " (t'juu', 98),\n",
       " (t'hapa', 98),\n",
       " (t'sheria', 98),\n",
       " (t'mwakani', 96),\n",
       " (t'sifuri', 95),\n",
       " (t'kubwa', 94),\n",
       " (t'jamii', 91),\n",
       " (t'wengine', 90),\n",
       " (t'iliyopita', 89),\n",
       " (t'tatu', 88),\n",
       " (t'kikwete', 85),\n",
       " (t'simba', 84),\n",
       " (t'kawaida', 83),\n",
       " (t'sita', 80),\n",
       " (t'duniani', 79),\n",
       " (t'hii', 77),\n",
       " (t'kimataifa', 77),\n",
       " (t'gani', 76),\n",
       " (t'ufisadi', 75),\n",
       " (t'zanzibar', 75),\n",
       " (t'polisi', 73),\n",
       " (t'wapi', 72),\n",
       " (t'ulaya', 72),\n",
       " (t'husika', 70),\n",
       " (t'bora', 70),\n",
       " (t'awali', 70),\n",
       " (t'pili', 69),\n",
       " (t'wote', 69),\n",
       " (t'dunia', 69),\n",
       " (t'mkubwa', 67),\n",
       " (t'bara', 67),\n",
       " (t'zake', 66),\n",
       " (t'chama', 66),\n",
       " (t'watanzania', 66),\n",
       " (t'habari', 66),\n",
       " (t'fedha', 65),\n",
       " (t'karibuni', 64),\n",
       " (t'mahakamani', 64),\n",
       " (t'nane', 64),\n",
       " (t'tano', 64),\n",
       " (t'nje', 64),\n",
       " (t'ujumla', 63),\n",
       " (t'kweli', 62),\n",
       " (t'kwake', 62),\n",
       " (t'watu', 62),\n",
       " (t'yoyote', 60),\n",
       " (t'kisiasa', 60),\n",
       " (t'binafsi', 60),\n",
       " (t'saba', 59),\n",
       " (t'uwanjani', 59),\n",
       " (t'makubwa', 58),\n",
       " (t'ushindi', 58),\n",
       " (t'juzi', 57),\n",
       " (t'ijayo', 57),\n",
       " (t'maisha', 56),\n",
       " (t'kwao', 56),\n",
       " (t'kiuchumi', 56),\n",
       " (t'nini', 56),\n",
       " (t'chake', 56),\n",
       " (t'usiku', 55),\n",
       " (t'mara', 55),\n",
       " (t'mrefu', 55),\n",
       " (t'nyumbani', 55),\n",
       " (t'maji', 54),\n",
       " (t'siku', 53),\n",
       " (t'tena', 53),\n",
       " (t'bunge', 53),\n",
       " (t'lake', 52),\n",
       " (t'dodoma', 52),\n",
       " (t'nne', 52),\n",
       " (t'yetu', 52),\n",
       " (t'soka', 52),\n",
       " (t'arusha', 50),\n",
       " (t'kilimo', 50),\n",
       " (t'kabisa', 50),\n",
       " (t'kenya', 50),\n",
       " (t'amani', 49),\n",
       " (t'wengi', 49),\n",
       " (t'la', 49),\n",
       " (t'coast', 49),\n",
       " (t'kuu', 48),\n",
       " (t'jumapili', 48),\n",
       " (t'mengine', 47),\n",
       " (t'salama', 47),\n",
       " (t'wenyewe', 47),\n",
       " (t'zote', 46),\n",
       " (t'nyingi', 46),\n",
       " (t'baadaye', 46),\n",
       " (t'chini', 46),\n",
       " (t'mwanza', 46),\n",
       " (t'kura', 45),\n",
       " (t'tofauti', 45),\n",
       " (t'kisheria', 45),\n",
       " (t'mengi', 44),\n",
       " (t'kidogo', 44),\n",
       " (t'ndani', 44),\n",
       " (t'daraja', 44),\n",
       " (t'hili', 43),\n",
       " (t'mwingine', 43),\n",
       " (t'mwenyewe', 42),\n",
       " (t'uchaguzi', 42),\n",
       " (t'mbaya', 42),\n",
       " (t'pinda', 42),\n",
       " (t'nyuma', 41),\n",
       " (t'kesho', 41),\n",
       " (t'tisa', 41),\n",
       " (t'matibabu', 40),\n",
       " (t'morogoro', 40),\n",
       " (t'mwisho', 40),\n",
       " (t'naye', 40),\n",
       " (t'rushwa', 40),\n",
       " (t'umeme', 40),\n",
       " (t'siasa', 39),\n",
       " (t'msingi', 39),\n",
       " (t'nayo', 39),\n",
       " (t'ndoa', 39),\n",
       " (t'zambia', 39),\n",
       " (t'fainali', 39),\n",
       " (t'rais', 38),\n",
       " (t'nzuri', 38),\n",
       " (t'uchumi', 38),\n",
       " (t'pwani', 38),\n",
       " (t'yako', 38),\n",
       " (t'ligi', 38),\n",
       " (t'alisisitiza', 37),\n",
       " (t'mungu', 37),\n",
       " (t'kufanya', 37),\n",
       " (t'elimu', 37),\n",
       " (t'huko', 37),\n",
       " (t'mbeya', 37),\n",
       " (t'mbele', 37),\n",
       " (t'mapema', 37),\n",
       " (t'watoto', 37),\n",
       " (t'haki', 36),\n",
       " (t'uliopita', 36),\n",
       " (t'mkuu', 36),\n",
       " (t'maximo', 36),\n",
       " (t'misri', 36),\n",
       " (t'mkononi', 35),\n",
       " (t'madarakani', 35),\n",
       " (t'binadamu', 35),\n",
       " (t'kutosha', 35),\n",
       " (t'wachezaji', 35),\n",
       " (t'sahihi', 34),\n",
       " (t'wowote', 34),\n",
       " (t'nao', 34),\n",
       " (t'haraka', 34),\n",
       " (t'majeruhi', 34),\n",
       " (t'jumamosi', 34),\n",
       " (t'madega', 34),\n",
       " (t'klabu', 34),\n",
       " (t'uhuru', 33),\n",
       " (t'mwaka', 33),\n",
       " (t'richmond', 33),\n",
       " (t'kulipwa', 33),\n",
       " (t'mashariki', 32),\n",
       " (t'tukio', 32),\n",
       " (t'jioni', 32),\n",
       " (t'mpya', 32),\n",
       " (t'mazuri', 32),\n",
       " (t'mchezo', 32),\n",
       " (t'mwakalebela', 32),\n",
       " (t'kitaifa', 31),\n",
       " (t'nani', 31),\n",
       " (t'chakula', 31),\n",
       " (t'wazi', 31),\n",
       " (t'mafanikio', 31),\n",
       " (t'united', 31),\n",
       " (t'stars', 31),\n",
       " (t'wavuni', 30),\n",
       " (t'shinyanga', 30),\n",
       " (t'uganda', 30),\n",
       " (t'phiri', 30),\n",
       " (t'muhimu', 29),\n",
       " (t'mashitaka', 29),\n",
       " (t'dini', 29),\n",
       " (t'umma', 29),\n",
       " (t'lipumba', 29),\n",
       " (t'wetu', 29),\n",
       " (t'kilimanjaro', 29),\n",
       " (t'pia', 29),\n",
       " (t'kusini', 29),\n",
       " (t'aliongeza', 28),\n",
       " (t'mfululizo', 28),\n",
       " (t'mazingira', 28),\n",
       " (t'kikubwa', 28),\n",
       " (t'gari', 28),\n",
       " (t'mtu', 28),\n",
       " (t'kuondoka', 28),\n",
       " (t'kodi', 28),\n",
       " (t'wakati', 28),\n",
       " (t'yangu', 28),\n",
       " (t'michezo', 28),\n",
       " (t'england', 28),\n",
       " (t'ubingwa', 28),\n",
       " (t'vyao', 27),\n",
       " (t'asubuhi', 27),\n",
       " (t'uzalishaji', 27),\n",
       " (t'vijana', 27),\n",
       " (t'timu', 27),\n",
       " (t'chan', 27),\n",
       " (t'mabao', 26),\n",
       " (t'bungeni', 26),\n",
       " (t'nzima', 26),\n",
       " (t'nidhamu', 26),\n",
       " (t'serikalini', 26),\n",
       " (t'halali', 26),\n",
       " (t'pekee', 26),\n",
       " (t'mitaa', 26),\n",
       " (t'vinginevyo', 26),\n",
       " (t'wangu', 26),\n",
       " (t'kati', 26),\n",
       " (t'umasikini', 25),\n",
       " (t'chiligati', 25),\n",
       " (t'muungano', 25),\n",
       " (t'walimu', 25),\n",
       " (t'ukweli', 25),\n",
       " (t'uhakika', 25),\n",
       " (t'yeyote', 25),\n",
       " (t'damu', 25),\n",
       " (t'vibaya', 25),\n",
       " (t'hatari', 25),\n",
       " (t'kondic', 25),\n",
       " (t'nyerere', 24),\n",
       " (t'kufunga', 24),\n",
       " (t'kijamii', 24),\n",
       " (t'mno', 24),\n",
       " (t'tatizo', 24),\n",
       " (t'liyumba', 24),\n",
       " (t'hasara', 24),\n",
       " (t'yote', 24),\n",
       " (t'vijijini', 24),\n",
       " (t'nguvu', 24),\n",
       " (t'mechi', 24),\n",
       " (t'wako', 24),\n",
       " (t'mahakama', 23),\n",
       " (t'wafanyakazi', 23),\n",
       " (t'tanga', 23),\n",
       " (t'hatua', 23),\n",
       " (t'demokrasia', 23),\n",
       " (t'wilaya', 23),\n",
       " (t'moto', 23),\n",
       " (t'kudumu', 23),\n",
       " (t'kukamilika', 23),\n",
       " (t'kumi', 23),\n",
       " (t'lolote', 23),\n",
       " (t'afya', 23),\n",
       " (t'wanawake', 23),\n",
       " (t'biashara', 23),\n",
       " (t'nk', 23),\n",
       " (t'marekani', 22),\n",
       " (t'temeke', 22),\n",
       " (t'wenzake', 22),\n",
       " (t'uwezo', 22),\n",
       " (t'mauaji', 22),\n",
       " (t'kagera', 22),\n",
       " (t'madini', 22),\n",
       " (t'muda', 22),\n",
       " (t'kadhaa', 22),\n",
       " (t'zijazo', 22),\n",
       " (t'zilizopita', 22),\n",
       " (t'ugenini', 22),\n",
       " (t'mgumu', 21),\n",
       " (t'sugar', 21),\n",
       " (t'macho', 21),\n",
       " (t'mwao', 21),\n",
       " (t'yenyewe', 21),\n",
       " (t'ngumu', 21),\n",
       " (t'sitta', 21),\n",
       " (t'kulevya', 21),\n",
       " (t'hivi', 21),\n",
       " (t'shule', 21),\n",
       " (t'fulani', 20),\n",
       " (t'mafisadi', 20),\n",
       " (t'sendeu', 20),\n",
       " (t'wakurugenzi', 20),\n",
       " (t'bibi', 20),\n",
       " (t'kushinda', 20),\n",
       " (t'upinzani', 20),\n",
       " (t'mabomu', 20),\n",
       " (t'vingine', 20),\n",
       " (t'st', 20),\n",
       " (t'mambo', 20),\n",
       " (t'jumatatu', 20),\n",
       " (t'zetu', 20),\n",
       " (t'shirikisho', 20),\n",
       " (t'lao', 19),\n",
       " (t'penalti', 19),\n",
       " (t'madaraka', 19),\n",
       " (t'huru', 19),\n",
       " (t'mchana', 19),\n",
       " (t'barabara', 19),\n",
       " (t'haya', 19),\n",
       " (t'ufumbuzi', 19),\n",
       " (t'hospitali', 19),\n",
       " (t'ndogo', 19),\n",
       " (t'mdogo', 19),\n",
       " (t'nafuu', 19),\n",
       " (t'kufa', 19),\n",
       " (t'wiki', 19),\n",
       " (t'kitu', 19),\n",
       " (t'muziki', 19),\n",
       " (t'pabaya', 18),\n",
       " (t'siri', 18),\n",
       " (t'marehemu', 18),\n",
       " (t'kipato', 18),\n",
       " (t'ltd', 18),\n",
       " (t'ikulu', 18),\n",
       " (t'uongozi', 18),\n",
       " (t'kuongeza', 18),\n",
       " (t'msaada', 18),\n",
       " (t'katiba', 18),\n",
       " (t'simu', 18),\n",
       " (t'watatu', 18),\n",
       " (t'yeye', 18),\n",
       " (t'sudan', 18),\n",
       " (t'mitaani', 18),\n",
       " (t'mkoa', 18),\n",
       " (t'mbali', 18),\n",
       " (t'mapato', 18),\n",
       " (t'imara', 18),\n",
       " (t'pale', 18),\n",
       " (t'mzuri', 18),\n",
       " (t'kisasa', 18),\n",
       " (t'sawa', 18),\n",
       " (t'mazoezi', 18),\n",
       " (t'uwanja', 18),\n",
       " (t'kusikilizwa', 17),\n",
       " (t'lini', 17),\n",
       " (t'kazini', 17),\n",
       " (t'risasi', 17),\n",
       " (t'kujifungua', 17),\n",
       " (t'usalama', 17),\n",
       " (t'upya', 17),\n",
       " (t'matatizo', 17),\n",
       " (t'rashid', 17),\n",
       " (t'miwili', 17),\n",
       " (t'mkataba', 17),\n",
       " (t'miundombinu', 17),\n",
       " (t'mali', 17),\n",
       " (t'mbagala', 17),\n",
       " (t'mwakyembe', 17),\n",
       " (t'rasmi', 17),\n",
       " (t'safi', 17),\n",
       " (t'jangwani', 17),\n",
       " (t'ushindani', 17),\n",
       " (t'matatu', 17),\n",
       " (t'wawili', 17),\n",
       " (t'wakubwa', 17),\n",
       " (t'mwenzake', 17),\n",
       " (t'mzima', 17),\n",
       " (t'mpira', 17),\n",
       " (t'ukimwi', 16),\n",
       " (t'makali', 16),\n",
       " (t'kasi', 16),\n",
       " (t'mrema', 16),\n",
       " (t'muafaka', 16),\n",
       " (t'slaa', 16),\n",
       " (t'pemba', 16),\n",
       " (t'dhamana', 16),\n",
       " (t'teknolojia', 16),\n",
       " (t'muhimbili', 16),\n",
       " (t'hadharani', 16),\n",
       " (t'chochote', 16),\n",
       " (t'msekwa', 16),\n",
       " (t'mwezi', 16),\n",
       " (t'tamaa', 16),\n",
       " (t'viongozi', 16),\n",
       " (t'jinai', 16),\n",
       " (t'kova', 16),\n",
       " (t'lukuvi', 16),\n",
       " (t'umaskini', 16),\n",
       " (t'hizi', 16),\n",
       " (t'cairo', 16),\n",
       " (t'prisons', 16),\n",
       " (t'ngassa', 16),\n",
       " (t'senegal', 16),\n",
       " (t'raha', 16),\n",
       " (t'pascal', 16),\n",
       " (t'hewa', 15),\n",
       " (t'kimya', 15),\n",
       " (t'maalum', 15),\n",
       " (t'chetu', 15),\n",
       " (t'ufundi', 15),\n",
       " (t'mapenzi', 15),\n",
       " (t'na', 15),\n",
       " (t'india', 15),\n",
       " (t'kutumika', 15),\n",
       " (t'njaa', 15),\n",
       " (t'mitatu', 15),\n",
       " (t'bure', 15),\n",
       " (t'mapinduzi', 15),\n",
       " (t'mwandosya', 15),\n",
       " (t'rwanda', 15),\n",
       " (t'wagonjwa', 15),\n",
       " (t'uongo', 15),\n",
       " (t'pamoja', 15),\n",
       " (t'adhabu', 15),\n",
       " (t'kote', 15),\n",
       " (t'vurugu', 15),\n",
       " (t'wabunge', 15),\n",
       " (t'magumu', 15),\n",
       " (t'kumalizika', 15),\n",
       " (t'ajira', 15),\n",
       " (t'kimaendeleo', 15),\n",
       " (t'wachache', 15),\n",
       " (t'city', 15),\n",
       " (t'sare', 15),\n",
       " (t'jamani', 15),\n",
       " (t'mawili', 15),\n",
       " (t'majaribio', 15),\n",
       " (t'chelsea', 15),\n",
       " (t'mashindano', 15),\n",
       " (t'ahly', 15),\n",
       " (t'kidini', 14),\n",
       " (t'mifugo', 14),\n",
       " (t'urahisi', 14),\n",
       " (t'taratibu', 14),\n",
       " (t'masomo', 14),\n",
       " (t'kuendelea', 14),\n",
       " (t'lissu', 14),\n",
       " (t'haramu', 14),\n",
       " (t'wenzao', 14),\n",
       " (t'ndege', 14),\n",
       " (t'kilango', 14),\n",
       " (t'zimbabwe', 14),\n",
       " (t'mwalimu', 14),\n",
       " (t'magari', 14),\n",
       " (t'wapya', 14),\n",
       " (t'makundi', 14),\n",
       " (t'raza', 14),\n",
       " (t'lazima', 14),\n",
       " (t'karibu', 14),\n",
       " (t'uingereza', 14),\n",
       " (t'mazishi', 14),\n",
       " (t'wewe', 14),\n",
       " (t'huduma', 14),\n",
       " (t'kigeni', 14),\n",
       " (t'kulia', 14),\n",
       " (t'said', 14),\n",
       " (t'busanda', 14),\n",
       " (t'italia', 14),\n",
       " (t'dalali', 14),\n",
       " (t'mashabiki', 14),\n",
       " (t'nyekundu', 14),\n",
       " (t'benchi', 14),\n",
       " (t'vipi', 14),\n",
       " (t'makini', 14),\n",
       " (t'alidai', 13),\n",
       " (t'ufanisi', 13),\n",
       " (t'hofu', 13),\n",
       " (t'mafuta', 13),\n",
       " (t'uchunguzi', 13),\n",
       " (t'kifo', 13),\n",
       " (t'mkono', 13),\n",
       " (t'zingine', 13),\n",
       " (t'kibiashara', 13),\n",
       " (t'mimi', 13),\n",
       " (t'ufaransa', 13),\n",
       " (t'jijini', 13),\n",
       " (t'kata', 13),\n",
       " (t'malaria', 13),\n",
       " (t'ushahidi', 13),\n",
       " (t'mkulo', 13),\n",
       " (t'jumatano', 13),\n",
       " (t'maana', 13),\n",
       " (t'nyumba', 13),\n",
       " (t'nafasi', 13),\n",
       " (t'congo', 13),\n",
       " (t'london', 13),\n",
       " (t'kaijage', 13),\n",
       " (t'kichwa', 13),\n",
       " (t'hispania', 13),\n",
       " (t'hamad', 12),\n",
       " (t'wanachama', 12),\n",
       " (t'nazo', 12),\n",
       " (t'silaha', 12),\n",
       " (t'burundi', 12),\n",
       " (t'nalo', 12),\n",
       " (t'baregu', 12),\n",
       " (t'mizigo', 12),\n",
       " (t'kugombea', 12),\n",
       " (t'wahusika', 12),\n",
       " (t'machozi', 12),\n",
       " (t'balali', 12),\n",
       " (t'utekelezaji', 12),\n",
       " (t'kanisa', 12),\n",
       " (t'mitano', 12),\n",
       " (t'wajawazito', 12),\n",
       " (t'makalla', 12),\n",
       " (t'utata', 12),\n",
       " (t'udanganyifu', 12),\n",
       " (t'dawa', 12),\n",
       " (t'utaratibu', 12),\n",
       " (t'masikini', 12),\n",
       " (t'geita', 12),\n",
       " (t'hiki', 12),\n",
       " (t'kali', 12),\n",
       " (t'mawasiliano', 12),\n",
       " (t'mvua', 12),\n",
       " (t'kingine', 12),\n",
       " (t'japan', 12),\n",
       " (t'akili', 12),\n",
       " (t'wafadhili', 12),\n",
       " (t'wasiwasi', 12),\n",
       " (t'njano', 12),\n",
       " (t'ilala', 12),\n",
       " (t'zamani', 12),\n",
       " (t'kujiamini', 12),\n",
       " (t'hassan', 12),\n",
       " (t'ajabu', 12),\n",
       " (t'ruvuma', 12),\n",
       " (t'mgosi', 12),\n",
       " (t'lyon', 12),\n",
       " (t'kufungwa', 12),\n",
       " (t'kucheza', 12),\n",
       " (t'usajili', 12),\n",
       " (t'majimaji', 11),\n",
       " (t'hai', 11),\n",
       " (t'kina', 11),\n",
       " (t'familia', 11),\n",
       " (t'selelii', 11),\n",
       " (t'vita', 11),\n",
       " (t'kwetu', 11),\n",
       " (t'madogo', 11),\n",
       " (t'duni', 11),\n",
       " (t'kujibu', 11),\n",
       " (t'mradi', 11),\n",
       " (t'chai', 11),\n",
       " (t'iringa', 11),\n",
       " (t'mohamed', 11),\n",
       " (t'tayari', 11),\n",
       " (t'kinondoni', 11),\n",
       " (t'fidia', 11),\n",
       " (t'kutokea', 11),\n",
       " (t'usoni', 11),\n",
       " (t'kutoka', 11),\n",
       " (t'upi', 11),\n",
       " (t'safari', 11),\n",
       " (t'kichwani', 11),\n",
       " (t'ile', 11),\n",
       " (t'kinyemela', 11),\n",
       " (t'kimoja', 11),\n",
       " (t'mingi', 11),\n",
       " (t'shida', 11),\n",
       " (t'kuishi', 11),\n",
       " (t'zitto', 11),\n",
       " (t'mwake', 11),\n",
       " (t'maumivu', 11),\n",
       " (t'endelevu', 11),\n",
       " (t'vitendo', 11),\n",
       " (t'chao', 11),\n",
       " (t'songea', 11),\n",
       " (t'wingi', 11),\n",
       " (t'balama', 11),\n",
       " (t'sekondari', 11),\n",
       " (t'shuleni', 11),\n",
       " (t'yaliyokusudiwa', 11),\n",
       " (t'suluhu', 11),\n",
       " (t'wakulima', 11),\n",
       " (t'unaendelea', 11),\n",
       " (t'dewji', 11),\n",
       " (t'tisini', 11),\n",
       " (t'baraza', 11),\n",
       " (t'mama', 11),\n",
       " (t'kukicha', 11),\n",
       " (t'kirahisi', 11),\n",
       " (t'mabingwa', 11),\n",
       " (t'serbia', 11),\n",
       " (t'kaseja', 11),\n",
       " (t'zenu', 11),\n",
       " (t'mabaya', 11),\n",
       " (t'wastani', 10),\n",
       " (t'makazi', 10),\n",
       " (t'kuwasaidia', 10),\n",
       " (t'maafa', 10),\n",
       " (t'ijumaa', 10),\n",
       " (t'mitihani', 10),\n",
       " (t'walipo', 10),\n",
       " (t'vodacom', 10),\n",
       " (t'kutajwa', 10),\n",
       " (t'leseni', 10),\n",
       " (t'wadogo', 10),\n",
       " (t'mfupi', 10),\n",
       " (t'ukonga', 10),\n",
       " (t'mshitakiwa', 10),\n",
       " (t'kujiandikisha', 10),\n",
       " (t'vingi', 10),\n",
       " (t'madhara', 10),\n",
       " (t'washitakiwa', 10),\n",
       " (t'kinana', 10),\n",
       " (t'mashaka', 10),\n",
       " (t'uzazi', 10),\n",
       " (t'mtanzania', 10),\n",
       " (t'pesa', 10),\n",
       " (t'ghafla', 10),\n",
       " (t'kuridhisha', 10),\n",
       " (t'samaki', 10),\n",
       " (t'wizi', 10),\n",
       " (t'utalii', 10),\n",
       " (t'millya', 10),\n",
       " (t'mwilini', 10),\n",
       " (t'kunywa', 10),\n",
       " (t'kufanyika', 10),\n",
       " (t'mashamba', 10),\n",
       " (t'albino', 10),\n",
       " (t'jiji', 10),\n",
       " (t'mtoto', 10),\n",
       " (t'karume', 10),\n",
       " (t'mkapa', 10),\n",
       " (t'iwezekanavyo', 10),\n",
       " (t'uamuzi', 10),\n",
       " (t'majina', 10),\n",
       " (t'ishirini', 10),\n",
       " (t'moshi', 10),\n",
       " (t'miguu', 10),\n",
       " (t'marekebisho', 10),\n",
       " (t'kocha', 10),\n",
       " (t'mapumziko', 10),\n",
       " (t'ghana', 10),\n",
       " (t'bao', 10),\n",
       " (t'ambani', 10),\n",
       " (t'kizuri', 10),\n",
       " (t'glory', 10),\n",
       " (t'vema', 10),\n",
       " (t'mazao', 9),\n",
       " (t'matokeo', 9),\n",
       " (t'mahojiano', 9),\n",
       " (t'ushauri', 9),\n",
       " (t'kinyume', 9),\n",
       " (t'maoni', 9),\n",
       " (t'kutisha', 9),\n",
       " (t'feki', 9),\n",
       " (t'kushiriki', 9),\n",
       " (t'bukombe', 9),\n",
       " (t'hujuma', 9),\n",
       " (t'uwekezaji', 9),\n",
       " (t'kamwe', 9),\n",
       " (t'mtwara', 9),\n",
       " (t'wanne', 9),\n",
       " (t'mawaziri', 9),\n",
       " (t'wanyonge', 9),\n",
       " (t'masharti', 9),\n",
       " (t'wanafunzi', 9),\n",
       " (t'ukabila', 9),\n",
       " (t'kubadilika', 9),\n",
       " (t'kienyeji', 9),\n",
       " (t'hawa', 9),\n",
       " (t'pembeni', 9),\n",
       " (t'basi', 9),\n",
       " (t'mwanzo', 9),\n",
       " (t'kusikojulikana', 9),\n",
       " (t'bil', 9),\n",
       " (t'maamuzi', 9),\n",
       " (t'walizopanda', 9),\n",
       " (t'ulinzi', 9),\n",
       " (t'malipo', 9),\n",
       " (t'ovyo', 9),\n",
       " (t'abiria', 9),\n",
       " (t'mbowe', 9),\n",
       " (t'bwana', 9),\n",
       " (t'yenu', 9),\n",
       " (t'sababu', 9),\n",
       " (t'wageni', 9),\n",
       " (t'mingine', 9),\n",
       " (t'ipasavyo', 9),\n",
       " (t'daladala', 9),\n",
       " (t'ziada', 9),\n",
       " (t'bakari', 9),\n",
       " (t'ujerumani', 9),\n",
       " (t'mijini', 9),\n",
       " (t'menejimenti', 9),\n",
       " (t'magomeni', 9),\n",
       " (t'uliopo', 9),\n",
       " (t'watano', 9),\n",
       " (t'huku', 9),\n",
       " (t'bendera', 9),\n",
       " (t'moyo', 9),\n",
       " (t'gerezani', 9),\n",
       " (t'makosa', 9),\n",
       " (t'unaotakiwa', 9),\n",
       " (t'sisi', 9),\n",
       " (t'fiti', 9),\n",
       " (t'mazoezini', 9),\n",
       " (t'trafford', 9),\n",
       " (t'ndimbo', 9),\n",
       " (t'africa', 9),\n",
       " (t'ferguson', 9),\n",
       " (t'kwangu', 9),\n",
       " (t'ule', 9),\n",
       " (t'jerome', 9),\n",
       " (t'marudiano', 9),\n",
       " (t'fa', 9),\n",
       " (t'marmo', 8),\n",
       " (t'alihoji', 8),\n",
       " (t'fujo', 8),\n",
       " (t'uhalifu', 8),\n",
       " (t'kawambwa', 8),\n",
       " (t'gazetini', 8),\n",
       " (t'mapambano', 8),\n",
       " (t'kesi', 8),\n",
       " (t'loliondo', 8),\n",
       " (t'misaada', 8),\n",
       " (t'makusudi', 8),\n",
       " (t'kuwasomesha', 8),\n",
       " (t'kupingwa', 8),\n",
       " (t'uzoefu', 8),\n",
       " (t'zilizopo', 8),\n",
       " (t'porini', 8),\n",
       " (t'kikazi', 8),\n",
       " (t'kiutendaji', 8),\n",
       " (t'mbadala', 8),\n",
       " (t'kizuiani', 8),\n",
       " (t'ardhini', 8),\n",
       " (t'ghasia', 8),\n",
       " (t'zahanati', 8),\n",
       " (t'upande', 8),\n",
       " (t'kuchanganyikiwa', 8),\n",
       " (t'tendwa', 8),\n",
       " (t'halisi', 8),\n",
       " (t'choo', 8),\n",
       " (t'jingine', 8),\n",
       " (t'mataifa', 8),\n",
       " (t'stahiki', 8),\n",
       " (t'umwagiliaji', 8),\n",
       " (t'taarifa', 8),\n",
       " (t'upatu', 8),\n",
       " (t'kosa', 8),\n",
       " (t'alasiri', 8),\n",
       " (t'sumu', 8),\n",
       " (t'jela', 8),\n",
       " (t'co', 8),\n",
       " (t'kuchangia', 8),\n",
       " (t'maelezo', 8),\n",
       " (t'mapya', 8),\n",
       " (t'anapokosea', 8),\n",
       " (t'kushoto', 8),\n",
       " (t'ardhi', 8),\n",
       " (t'baadae', 8),\n",
       " (t'yaliyopo', 8),\n",
       " (t'ukiendelea', 8),\n",
       " (t'matumizi', 8),\n",
       " (t'urais', 8),\n",
       " (t'sweden', 8),\n",
       " (t'maalumu', 8),\n",
       " (t'bandia', 8),\n",
       " (t'kikamilifu', 8),\n",
       " (t'hapana', 8),\n",
       " (t'mosi', 8),\n",
       " (t'kadhi', 8),\n",
       " (t'asili', 8),\n",
       " (t'kuongezeka', 8),\n",
       " (t'nagu', 8),\n",
       " (t'gesi', 8),\n",
       " (t'kipindupindu', 8),\n",
       " (t'holela', 8),\n",
       " (t'uwazi', 8),\n",
       " (t'china', 8),\n",
       " (t'njema', 8),\n",
       " (t'wanaume', 8),\n",
       " (t'urafiki', 8),\n",
       " (t'kimapenzi', 8),\n",
       " (t'nguruwe', 8),\n",
       " (t'manne', 8),\n",
       " (t'kipya', 8),\n",
       " (t'nyinginezo', 8),\n",
       " (t'dogo', 8),\n",
       " (t'televisheni', 8),\n",
       " (t'nigeria', 8),\n",
       " (t'bukoba', 8),\n",
       " (t'mpango', 8),\n",
       " (t'mtibwa', 8),\n",
       " (t'goli', 8),\n",
       " (t'mwalusako', 8),\n",
       " (t'villa', 8),\n",
       " (t'mchezaji', 8),\n",
       " (t'kaduguda', 8),\n",
       " (t'kifamilia', 8),\n",
       " (t'tenga', 8),\n",
       " (t'emirates', 8),\n",
       " (t'msimu', 8),\n",
       " (t'letu', 8),\n",
       " (t'kumb', 8),\n",
       " (t'edward', 8),\n",
       " (t'wapendwa', 8),\n",
       " (t'ham', 8),\n",
       " (t'cannavaro', 8),\n",
       " (t'thebathini', 8),\n",
       " (t'tff', 8),\n",
       " (t'kaskazini', 7),\n",
       " (t'rangi', 7),\n",
       " (t'chuki', 7),\n",
       " (t'seleman', 7),\n",
       " (t'kutwa', 7),\n",
       " (t'maziko', 7),\n",
       " (t'juma', 7),\n",
       " (t'joto', 7),\n",
       " (t'lupembe', 7),\n",
       " (t'kuandika', 7),\n",
       " (t'kulala', 7),\n",
       " (t'kupatikana', 7),\n",
       " (t'shaka', 7),\n",
       " (t'zitachukuliwa', 7),\n",
       " (t'kupinduka', 7),\n",
       " (t'tupu', 7),\n",
       " (t'uendeshaji', 7),\n",
       " (t'kuzuia', 7),\n",
       " (t'huyu', 7),\n",
       " (t'kujeruhiwa', 7),\n",
       " (t'mchele', 7),\n",
       " (t'kusimama', 7),\n",
       " (t'hatia', 7),\n",
       " (t'ibrahim', 7),\n",
       " (t'kelele', 7),\n",
       " (t'kuvuna', 7),\n",
       " (t'salim', 7),\n",
       " (t'kapuya', 7),\n",
       " (t'shelukindo', 7),\n",
       " (t'kuchukuliwa', 7),\n",
       " (t'papa', 7),\n",
       " (t'hatiani', 7),\n",
       " (t'vipimo', 7),\n",
       " (t'mushi', 7),\n",
       " (t'tamko', 7),\n",
       " (t'lenyewe', 7),\n",
       " (t'benki', 7),\n",
       " (t'utafiti', 7),\n",
       " (t'amekufa', 7),\n",
       " (t'bastola', 7),\n",
       " (t'mpendazoe', 7),\n",
       " (t'kujiuzulu', 7),\n",
       " (t'kutolewa', 7),\n",
       " (t'wakuu', 7),\n",
       " (t'matunda', 7),\n",
       " (t'kulipuka', 7),\n",
       " (t'apumzike', 7),\n",
       " (t'kikatili', 7),\n",
       " (t'ilivyozoeleka', 7),\n",
       " (t'ally', 7),\n",
       " (t'kuhojiwa', 7),\n",
       " (t'maadili', 7),\n",
       " (t'uturuki', 7),\n",
       " (t'halmashauri', 7),\n",
       " (t'keshokutwa', 7),\n",
       " (t'mwekezaji', 7),\n",
       " (t'mbegu', 7),\n",
       " (t'kuigwa', 7),\n",
       " (t'nyongeza', 7),\n",
       " (t'kulalamika', 7),\n",
       " (t'nacho', 7),\n",
       " (t'bahari', 7),\n",
       " (t'malengo', 7),\n",
       " (t'kusema', 7),\n",
       " (t'kwako', 7),\n",
       " (t'nnauye', 7),\n",
       " (t'wateja', 7),\n",
       " (t'mokiwa', 7),\n",
       " (t'kumuua', 7),\n",
       " (t'mahanga', 7),\n",
       " (t'faida', 7),\n",
       " (t'mtumiaji', 7),\n",
       " (t'mramba', 7),\n",
       " (t'hasa', 7),\n",
       " (t'waaminifu', 7),\n",
       " (t'rahisi', 7),\n",
       " (t'jumuiya', 7),\n",
       " (t'rumande', 7),\n",
       " (t'mikoani', 7),\n",
       " (t'kombe', 7),\n",
       " (t'kike', 7),\n",
       " (t'masoko', 7),\n",
       " (t'jeshi', 7),\n",
       " (t'magharibi', 7),\n",
       " (t'kusaidia', 7),\n",
       " (t'malawi', 7),\n",
       " (t'mwema', 7),\n",
       " (t'kula', 7),\n",
       " (t'zipi', 7),\n",
       " (t'lukombe', 7),\n",
       " (t'shein', 7),\n",
       " (t'magesa', 7),\n",
       " (t'juni', 7),\n",
       " (t'washindi', 7),\n",
       " (t'itakuwaje', 7),\n",
       " (t'jina', 7),\n",
       " (t'singida', 7),\n",
       " (t'kuanza', 7),\n",
       " (t'kiufundi', 7),\n",
       " (t'bongo', 7),\n",
       " (t'maambukizi', 7),\n",
       " (t'kondomu', 7),\n",
       " (t'jackson', 7),\n",
       " (t'norway', 7),\n",
       " (t'pointi', 7),\n",
       " (t'milan', 7),\n",
       " (t'hiddink', 7),\n",
       " (t'liverpool', 7),\n",
       " (t'wakongwe', 7),\n",
       " (t'kambini', 7),\n",
       " (t'angola', 7),\n",
       " (t'kiungo', 7),\n",
       " (t'kisoka', 7),\n",
       " (t'vipaji', 7),\n",
       " (t'khartoum', 7),\n",
       " (t'jumanne', 7),\n",
       " (t'salome', 7),\n",
       " (t'ngozi', 7),\n",
       " (t'kubahatisha', 7),\n",
       " (t'arsenal', 7),\n",
       " (t'benitez', 7),\n",
       " (t'zilizobaki', 7),\n",
       " (t'pengo', 6),\n",
       " (t'ujumbe', 6),\n",
       " (t'kawawa', 6),\n",
       " (t'dhaifu', 6),\n",
       " (t'ushirikiano', 6),\n",
       " (t'majambazi', 6),\n",
       " (t'amelala', 6),\n",
       " (t'butiku', 6),\n",
       " (t'iliyopo', 6),\n",
       " (t'kujitegemea', 6),\n",
       " (t'dua', 6),\n",
       " (t'wanayoishi', 6),\n",
       " (t'mkali', 6),\n",
       " (t'alilalamika', 6),\n",
       " (t'manunuzi', 6),\n",
       " (t'baharini', 6),\n",
       " (t'wazanzibar', 6),\n",
       " (t'kushitakiwa', 6),\n",
       " (t'mtandao', 6),\n",
       " (t'jadi', 6),\n",
       " (t'kibaha', 6),\n",
       " (t'boniface', 6),\n",
       " (t'vitambulisho', 6),\n",
       " (t'khalifa', 6),\n",
       " (t'kundecha', 6),\n",
       " (t'rumanyika', 6),\n",
       " (t'nyange', 6),\n",
       " (t'unguja', 6),\n",
       " (t'utajiri', 6),\n",
       " (t'walizonazo', 6),\n",
       " (t'mahitaji', 6),\n",
       " (t'mashahidi', 6),\n",
       " (t'malecela', 6),\n",
       " (t'kuona', 6),\n",
       " (t'kinachoendelea', 6),\n",
       " (t'makuu', 6),\n",
       " (t'ufafanuzi', 6),\n",
       " (t'limited', 6),\n",
       " (t'gama', 6),\n",
       " (t'tanganyika', 6),\n",
       " (t'maradhi', 6),\n",
       " (t'haujakamilika', 6),\n",
       " (t'tabora', 6),\n",
       " (t'mtihani', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_occurance_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fb0ce-1fb4-4991-a17d-e31272f2bb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

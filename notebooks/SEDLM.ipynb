{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62bcb026-ef22-4f3b-ab45-fb6653082296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3812e208-2b25-465a-bba4-4349be507b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for the SED\n",
    "from marynlp.modules.source import save_model_from_google_bucket\n",
    "from marynlp.utils.storage import download, get_bucket\n",
    "\n",
    "\n",
    "# Download the morphmeme file\n",
    "bucket = get_bucket(\"../resources/mary_africa_credentials_key.json\", \"marynlp-private\")\n",
    "morph_template_file = save_model_from_google_bucket(\"models/sed_morpheme_template.txt\", bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d112c8e-173d-499d-9234-71e6e109b804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/iam-kevin/.marynlp/store/models/sed_morpheme_template.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_template_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5c048d-aa0c-4b21-94db-d72801346029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the words\n",
    "from marynlp import funcutils as f\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List, Union\n",
    "import os\n",
    "import re\n",
    "\n",
    "def split_by_space(text: str) -> List[str]:\n",
    "    \"\"\"Split a text to word strings\n",
    "    \n",
    "    Example\n",
    "    \"Lorem ipsum\" -> [ 'Lorem', 'ipsum' ] \n",
    "    \"\"\"\n",
    "    return re.split(r\"\\s+\", text)\n",
    "\n",
    "def ignore_rules(text: str) -> bool:\n",
    "    return not (text.find(\"<text\") > -1 or text.find(\"</text>\") > -1)\n",
    "\n",
    "def should_be_longer_than_20(text: str) -> bool:\n",
    "    return len(text) > 20\n",
    "\n",
    "@f.filterBy(f.rules(ignore_rules, should_be_longer_than_20))\n",
    "def read_file(file_path) -> List[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "@f.forEach(str, type_=set)\n",
    "@f.filterBy(lambda s: len(s.strip()) > 0)\n",
    "@f.flowBy(split_by_space)\n",
    "@f.forEach(lambda s: s.strip())\n",
    "def get_unique_words_from_shu_file(file_path: os.PathLike):\n",
    "    return tqdm(read_file(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1b9eb4-5b08-49e3-acb4-a7b08b46ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from experimental.sed import MorphologyAnalyzer\n",
    "from marynlp import funcutils as f \n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "def break_word(word: str, analyzer: MorphologyAnalyzer) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "# Might not want to add this since it's pytorch specific?\n",
    "class MorphemeDataset(Dataset):\n",
    "    def __init__(self, word_iter: Iterable[str], break_word: Callable):\n",
    "        self.break_word = break_word\n",
    "        self.wml = list(word_iter)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        return self.break_word(self.wml[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef631a2-202a-4845-98b4-e35ef031f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_morphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f67cc8a-1670-46f1-ad0f-2fc8bcc0d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 763155.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from marynlp.text.data import Vocab, morph\n",
    "from marynlp.text import formatter as fmt\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from marynlp.utils import process as p\n",
    "\n",
    "unique_words = get_unique_words_from_shu_file(\"../resources/data/hcs-na-v2/new-mat/news/alasiri-2009.shu\")\n",
    "analyzer = MorphologyAnalyzer(morph_template_file)\n",
    "\n",
    "# start with remove punctuations\n",
    "clean_text = f.calls(fmt.remove_punctuations, fmt.white_space_cleaning, fmt.lowercase)\n",
    "\n",
    "@f.apply(f.calls(chain.from_iterable, set))\n",
    "def get_unique_morphemes(words: Iterable[str]):\n",
    "    breaker = f.calls(clean_text, f.partial(break_word, analyzer=analyzer))\n",
    "    for w in tqdm(words):\n",
    "        try:\n",
    "            yield breaker(w)\n",
    "        except Exception as e:\n",
    "            print(\"Unable to break word: '%s'\" % w)\n",
    "            raise e\n",
    "\n",
    "            \n",
    "# unique_morphemes = get_unique_morphemes(unique_words)\n",
    "# morpheme_vocab = Vocab(unique_morphemes)\n",
    "# word_vocab = Vocab(map(clean_text, unique_words))\n",
    "# dataset = MorphemeDataset(unique_words, break_word=f.partial(break_word, analyzer=analyzer))\n",
    "# clean_text(\"sisi?,!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff18372-28cf-4e2a-a450-a58edd5a2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  = f.partial(break_word, analyzer=analyzer)\n",
    "# get_unique_morphemes(\"kevin james\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7540a5-8e3c-4f69-8526-1fc0d53f4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This must be updated the data\n",
    "#  in accordance to the data that is used to train the data\n",
    "\n",
    "# word_vocab, morpheme_vocab # Vocab([ , 0..., zwa], len=569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8910cd9-43d9-4553-917f-cc0516fe8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.sed.nn import SEDLanguageModel\n",
    "from experimental.sed.modules.embeddings import SEDWordEmbeddings\n",
    "\n",
    "# swe = SEDWordEmbeddings(\n",
    "#     morpheme_vocab.size,\n",
    "#     embedding_dim=100,\n",
    "#     hidden_dim=32\n",
    "# )\n",
    "\n",
    "def lm_pad_collate(batch):\n",
    "    x, x_len = [], []\n",
    "    \n",
    "    x_len = [m.shape[0] for m in batch]\n",
    "    max_len = max(x_len)\n",
    "    \n",
    "    x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "    xx = torch.stack([torch.from_numpy(x).long() for x in x_batch_])\n",
    "\n",
    "    return xx, torch.as_tensor(x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec098054-2507-4c5f-9b1e-ae75eb11dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from marynlp.text.data import token\n",
    "from marynlp.text.processor import TokenEncoder\n",
    "\n",
    "from marynlp.text import formatter as fmt\n",
    "from itertools import chain\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "\n",
    "def split_text_by_space(text: str) -> Tuple[str]:\n",
    "    return re.split(r'\\s+', fmt.white_space_cleaning(text)) \n",
    "\n",
    "\n",
    "@f.apply(list)\n",
    "def text_pad_sequence(word_sequence: Iterable[Any], padding_length: int, pad_marker: Any):\n",
    "    if len(word_sequence) >= padding_length:\n",
    "        return word_sequence\n",
    "    \n",
    "    return chain.from_iterable((word_sequence, [pad_marker] * (padding_length - len(word_sequence))))\n",
    "\n",
    "@f.apply(tuple)\n",
    "@f.apply(lambda o: zip(*o))\n",
    "def width_pad_sequence(var_sequences: Iterable[List[Any]], padding_length: int, pad_marker: Any):\n",
    "    for var_seq in var_sequences:\n",
    "        out = tuple(var_seq)\n",
    "        yield tuple(chain.from_iterable((out, [pad_marker] * (padding_length - len(var_seq))))), len(out)\n",
    "\n",
    "        \n",
    "# PAD_TOKEN = token('<PAD>')\n",
    "\n",
    "\n",
    "# morph_encoder = TokenEncoder(morpheme_vocab)\n",
    "# word_to_morpheme = f.partial(break_word, analyzer=analyzer)\n",
    "# encode_morpheme = f.apply(np.array)(f.forEach(morph_encoder.encode)(word_to_morpheme))\n",
    "\n",
    "\n",
    "# def pad_collateV2(batch):\n",
    "#     \"\"\"NOTE: This is the modified version from the package\"\"\"\n",
    "#     x, x_len = [], []\n",
    "    \n",
    "#     x_len = [m.shape[0] for m in batch]\n",
    "#     max_len = max(x_len)\n",
    "    \n",
    "#     x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "#     xx = torch.stack([torch.from_numpy(x).long() for x in x_batch_])\n",
    "\n",
    "#     return xx, torch.as_tensor(x_len)\n",
    "\n",
    "\n",
    "\n",
    "# @f.apply(list)\n",
    "def morph_sentence_pad_collate(sentences: List[str], padding_idx: int = 0):\n",
    "    word_sequences = list(map(split_text_by_space, sentences))\n",
    "#     print(word_sequences)\n",
    "    \n",
    "    # padd each sequence\n",
    "    out_list = [list(map(word_to_morpheme, b)) for b in word_sequences]\n",
    "    print(out_list)\n",
    "\n",
    "    # get the longest character sequence in the entire batch\n",
    "    to_pad_morph_length = len(max(list(map(f.partial(max, key=len), out_list)), key=len))\n",
    "    to_pad_word_length = max(map(len, out_list))\n",
    "\n",
    "    morph_padd_function = f.partial(width_pad_sequence, padding_length=to_pad_morph_length, pad_marker=padding_idx)\n",
    "    \n",
    "#         print(\"m_idx_seq\", padd_function(m_idx_seq))\n",
    "    \n",
    "    # hold the number of words and morphme pair count\n",
    "    word_morphemes_count = []\n",
    "    words_tensors = []\n",
    "    words_c = []  # number of words in a sentence\n",
    "\n",
    "    for b in out_list:\n",
    "        word_list = [ list(map(morph_encoder.encode, b_)) for b_ in b ]\n",
    "#         print(word_list)\n",
    "        \n",
    "        padded_morph_sequence, lengths = morph_padd_function(word_list)\n",
    "        \n",
    "#         c, _ = morpheme_pad_collate(list(map(encode_morpheme, b)))\n",
    "#         word_count, longest_token_len = c.shape[0], c.shape[-1]\n",
    "\n",
    "        # grid padding\n",
    "        word_count = len(word_list)\n",
    "        padd_words_count = to_pad_word_length - word_count\n",
    "        out = (tuple([0] * to_pad_morph_length) for _ in range(padd_words_count))    \n",
    "        \n",
    "        words_tensors.append(np.array(list(chain(padded_morph_sequence, out))))\n",
    "#         word_morphemes_count.append(np.array(tuple(chain(lengths, [0] * padd_words_count))))\n",
    "        word_morphemes_count.append(\n",
    "            tuple(chain(\n",
    "                lengths, \n",
    "                [0] * padd_words_count # padding\n",
    "            )))\n",
    "    \n",
    "        words_c.append(word_count)\n",
    "\n",
    "    padded_length_tensors = np.array(word_morphemes_count)\n",
    "#         # get the remaining shape\n",
    "# #         print(c)\n",
    "#     print(out)\n",
    "                             \n",
    "    # padd the sentence objects\n",
    "#     padded_length_tensors = pad_sequence(word_morphemes_count, batch_first=True)\n",
    "\n",
    "    return np.array(words_tensors), np.array(padded_length_tensors), np.array(words_c)\n",
    "\n",
    "# def padded_input(pad_idx: int, padding_length: int):\n",
    "#     return np.array([pad_idx] * padding_length)\n",
    "\n",
    "\n",
    "# t, l = morpheme_pad_collate(list(MorphemeDataset([\"mama\", \"anakuja\"], encode_morpheme))); t\n",
    "\n",
    "# split sentence\n",
    "# lm = DataLoader()\n",
    "# sentences = [\"mama anakuja na mama\", \"mwalimu alikuwa anafundisha\"]\n",
    "# t, mcs, wc = morph_sentence_pad_collate(sentences); \n",
    "# t.shape, mcs.shape, wc.shape\n",
    "\n",
    "# # set_pad = \n",
    "# # print(set_pad)\n",
    "# t[1], mcs[1], wc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8242d7-4dec-4650-ac3b-b3c8e25a35bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<experimental.sed.morphology.MorphologyAnalyzer at 0x7fa50898fd10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e1d1c-2187-46dd-b961-5ec76d2c676e",
   "metadata": {},
   "source": [
    "### Showing Nelson how its done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b362b7eb-e126-4f1c-a30d-3f98a31da334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp import funcutils as f\n",
    "from typing import Tuple\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "def split_text_by_space(text: str) -> Tuple[str]:\n",
    "    return re.split(r'\\s+', fmt.white_space_cleaning(text)) \n",
    "\n",
    "@f.forEach(f.calls(lambda s: s.strip(), split_text_by_space))\n",
    "def get_word_sequences_from_txt_file(file_path: os.PathLike):\n",
    "    return tqdm(read_file(file_path))\n",
    "\n",
    "@f.forEach(str, type_=set)\n",
    "def get_unique_words_from_txt_file(file_path: os.PathLike):\n",
    "    return chain.from_iterable(get_word_sequences_from_txt_file(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55700e98-9b36-4371-b94c-97df4a980bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 93767.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3347/3347 [00:00<00:00, 97418.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29343/29343 [01:18<00:00, 374.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Vocab([UNK, a..., zurich], len=29343), Vocab([a, aa..., zzy], len=5230))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marynlp.text.data import Vocab\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "unique_words = get_unique_words_from_txt_file(\"../resources/nelson/train.txt\") | get_unique_words_from_txt_file(\"../resources/nelson/valid.txt\")\n",
    "# unique_words = set(list(chain.from_iterable(list(map(split_text_by_space, sentences)))))\n",
    "# print(unique_words)\n",
    "unique_morphemes = get_unique_morphemes(unique_words)\n",
    "\n",
    "# print(unique_morphemes)\n",
    "word_vocab = Vocab(unique_words)\n",
    "morpheme_vocab = Vocab(unique_morphemes)\n",
    "\n",
    "\n",
    "word_vocab, morpheme_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48d35c41-8ac1-4871-bca9-2f7c805466f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.processor import PaddedTokenEncoder, \n",
    "\n",
    "word_tokenizer = PaddedTokenEncoder(word_vocab)\n",
    "morpheme_tokenizer = PaddedTokenEncoder(morpheme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c7fcbfe-740d-4915-b82c-272f52eae0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345),\n",
       " TokenEncoder(<PAD>=0, <UNK>=1..., zzy=5231, l=5232))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer, morpheme_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d893e78-7b0f-4522-bfff-90063e73ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the embeddings\n",
    "import numpy as np\n",
    "from experimental.sed.modules.embeddings import SEDWordEmbeddings\n",
    "\n",
    "def break_word(word: str, analyzer: MorphologyAnalyzer) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "analyzer = MorphologyAnalyzer(morph_template_file)\n",
    "\n",
    "\n",
    "swe = SEDWordEmbeddings(\n",
    "    morpheme_tokenizer.size,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=32\n",
    ")\n",
    "\n",
    "word_to_morpheme = f.partial(break_word, analyzer=analyzer)\n",
    "encode_morpheme = f.apply(np.array)(f.forEach(morpheme_tokenizer.encode)(word_to_morpheme))\n",
    "morpheme_dataset = MorphemeDataset(word_tokenizer.get_tokens(), break_word=encode_morpheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f38f9c30-1937-4d2b-9095-07928585c4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [01:55<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i in morpheme_dataset:\n",
    "#     print(i)\n",
    "embeddings = swe.fit(morpheme_dataset)\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40ee86ad-446f-48d2-8815-62e64253b055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29345, 100]),\n",
       " TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, word_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b83f2-c2a7-4d2d-9f4a-09d322db4237",
   "metadata": {},
   "source": [
    "### Finally, the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72261466-7f04-4d0d-aa4b-4852689eb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def next_token_generate(word_sequence: Iterable[Any]) -> Tuple[List[Any], Any]:\n",
    "    word_sequence = tuple(word_sequence)\n",
    "    return word_sequence[:-1], word_sequence[-1]\n",
    "            \n",
    "def next_token_sequence_generator(word_sequence: Iterable[Any]) -> Iterable[Tuple[List[Any], Any]]:\n",
    "    return [next_token_generate(word_sequence)]\n",
    "\n",
    "from functools import wraps\n",
    "from itertools import chain\n",
    "\n",
    "def flow(iterator_fn):\n",
    "    @wraps(iterator_fn)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return chain.from_iterable(iterator_fn(*args, **kwargs))\n",
    "    return wrapper\n",
    "\n",
    "@f.apply(list)\n",
    "@flow\n",
    "def generate_word_next_token_sequence(word_sequences: Iterable[List[str]]):\n",
    "    for w in word_sequences:\n",
    "        yield next_token_sequence_generator(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4aa2857-f754-4fab-9708-e3eea4285ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections.abc import Callable\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, word_sequences: Iterable[Tuple[List[str], str]], word_encode: Callable = None):\n",
    "        self.ls = word_sequences\n",
    "        self.encode = word_encode\n",
    "    \n",
    "    def __getitem__(self, ix: int):\n",
    "        word_seq, next_token = self.ls[ix]\n",
    "        return list(map(self.encode, word_seq)), self.encode(next_token)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d8976b63-41b7-487e-8314-3ac6fdc0024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.apply(list)\n",
    "def set_proper_UNK_in_word_sequence(word_sequence):\n",
    "    \"\"\"Fixes the token by replacing UNK with <UNK> (that's a proper unknown token)\"\"\"\n",
    "    for word in word_sequence:\n",
    "        if word == 'UNK':\n",
    "            yield token('<UNK>')\n",
    "            continue\n",
    "        yield token(word)\n",
    "            \n",
    "get_better_sequences_from_txt_file = f.forEach(set_proper_UNK_in_word_sequence)(get_word_sequences_from_txt_file)\n",
    "# word_sequences = get_better_sequences_from_txt_file(\"../resources/nelson/train.txt\")\n",
    "\n",
    "generate_word_sequences_from_file = f.apply(generate_word_next_token_sequence)(get_better_sequences_from_txt_file)\n",
    "build_dataset_from_file = f.apply(f.partial(LMDataset, word_encode=word_tokenizer.encode))(generate_word_sequences_from_file)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4a8b251-1a47-49ff-95e3-aa31f3e535eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 99548.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3347/3347 [00:00<00:00, 90860.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = build_dataset_from_file(\"../resources/nelson/train.txt\") \n",
    "val_dataset = build_dataset_from_file(\"../resources/nelson/valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a18968b-8090-43df-abb6-aa65f755f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_pad_collate(batch: List[\n",
    "    Tuple[ List[int], int ]\n",
    "]):\n",
    "    out, out_len = tuple(zip(*batch))\n",
    "    padded_output = pad_sequence(list(map(torch.tensor, out)), batch_first=True)\n",
    "    return (padded_output, torch.tensor(out_len)) # word_sequence, next_token\n",
    "\n",
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4761ef41-8b52-4e16-9a3f-a73c4dc868f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from time import time\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=lm_pad_collate)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=lm_pad_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f2184069-9af5-486a-a3c5-212fa3623174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TokenEncoder(<PAD>=0, <UNK>=1..., zurich=29344, l=29345), 41515)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenizer, len(word_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ba314383-7a2e-4ca9-8a2b-dc26afab118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, model_name: str, checkpoint_folder_path: str):\n",
    "        \"\"\"Setting the configuration for training the model properly\"\"\"\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        checkpoint_folder_path = Path(checkpoint_folder_path)\n",
    "        assert not Path(checkpoint_folder_path).exists(), \"The folder exists. Delete the folder or enter a new path\"\n",
    "        \n",
    "        # make the folder\n",
    "        checkpoint_folder_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.checkpoint_folder_path = str(checkpoint_folder_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def result_from_checkpoint(cls, model):\n",
    "        \"\"\"Reset from the checking\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def save_model_checkpoint(self, info: str):\n",
    "        saved_model_path = str(Path(self.checkpoint_folder_path).joinpath(\"{}.{}.pth\".format(self.model_name, info)))\n",
    "        print(\"Saved! (not really):\", saved_model_path)\n",
    "    \n",
    "    def _train_epoch_step(self, train_dataloader, criterion, optimizer, epoch=1, verbose=100):\n",
    "        start_time = time()\n",
    "        t_batch = len(train_dataloader)\n",
    "        \n",
    "        for batch_idx, _data in enumerate(train_dataloader):\n",
    "            word_sequences, y = _data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(word_sequences)\n",
    "            loss = self.criterion(output, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % verbose == 0 or batch_idx + 1 == t_batch:\n",
    "                print('Train Epoch: {:03d} [{:03d}/{:03d} ({:.0f}%)]\\tLoss: {:.6f}\\tPerplexity: {:5.2f}\\telapsed: {:.2f} mins'.format(\n",
    "                    epoch, (batch_idx + 1), t_batch,\n",
    "                    100. * ((batch_idx + 1) / t_batch), loss.item(), np.exp(loss.item()), (time() - start_time)/60)\n",
    "                )\n",
    "    \n",
    "    def _val_epoch_step(self, val_dataloader, criterion):\n",
    "        print(\"-\"*60)\n",
    "\n",
    "        self.model.eval()    \n",
    "        test_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for I, _data in enumerate(val_dataloader):\n",
    "                word_sequences, y = _data\n",
    "                output = self.model(word_sequences)\n",
    "                loss = torch.sum(torch.stack([self.criterion(out, y) for out,y in zip([output], y)]))\n",
    "                test_loss += loss.item() / len(val_dataloader)\n",
    "\n",
    "        print('Test set: Average loss: {:.4f} | Perplexity: {:8.2f}\\n'.format(test_loss, np.exp(test_loss)))\n",
    "        return round(test_loss, 4)#, self.model.state_dict()\n",
    "    \n",
    "    def train(self, \n",
    "                train_dataloader, \n",
    "                val_dataloader, \n",
    "                epochs=EPOCHS, \n",
    "                verbose=100,\n",
    "                criterion=F.cross_entropy,\n",
    "                lr=.01\n",
    "             ):\n",
    "        self.model.train()\n",
    "        \n",
    "        optimizer=optim.AdamW(model.parameters(), lr=lr)\n",
    "        accum_loss = torch.tensor(float('inf'))\n",
    "        start_exp_time = time()\n",
    "        \n",
    "        data_len = len(train_dataloader.dataset)\n",
    "        t_batch = len(train_dataloader)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # making the training step\n",
    "            self._train_epoch_step(train_dataloader, criterion, optimizer, epoch=epoch, verbose=verbose)\n",
    "            \n",
    "            test_loss = self._val_epoch_step(val_dataloader, criterion)\n",
    "            \n",
    "            if test_loss < accum_loss:\n",
    "                # Save the model\n",
    "                self.save_model_checkpoint(epoch + \"_loss_{:.02f}\".format(test_loss))\n",
    "                accum_loss = test_loss\n",
    "                stop_eps = 0\n",
    "            else:\n",
    "                stop_eps += 1\n",
    "                \n",
    "            print(\"-\"*60)\n",
    "            epoch_time_elapsed = (time() - start_time)/60\n",
    "            print(\"End of epoch {}/{} | Time elapsed: {} mins | val_loss: {}\".format(epoch, epochs, epoch_time_elapsed, test_loss))\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            # Early stopping\n",
    "            if stop_eps >= early_stop:\n",
    "                # Save the model\n",
    "                self.save_model_checkpoint(epoch + \"_early\")\n",
    "                break\n",
    "\n",
    "    def save_model_checkpoint(self):\n",
    "        best_model, accum_loss = self.model_checkpoints[-1]\n",
    "\n",
    "        if self.save_path is not None and not ModelsConfig.lang_mod:\n",
    "            torch.save(best_model, Path(self.save_path).joinpath(f'{accum_loss}_SAM.pth'))\n",
    "        \n",
    "        if self.save_path is not None and ModelsConfig.lang_mod:\n",
    "            torch.save(best_model, Path(self.save_path).joinpath(f'{accum_loss}_lm.pth'))\n",
    "\n",
    "    def load_model_checkpoint(self, model_weights):\n",
    "        self.model.load_state_dict(torch.load(Path(model_weights)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19b165-36c9-4b47-9429-d66de0f14587",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5024b430-4f3e-45d8-b067-5d8677be2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual training going on\n",
    "!rm -rf ./sed_checkpoint\n",
    "\n",
    "model = SEDLanguageModel(embeddings)\n",
    "trainer = Trainer(model, \"sed_language_model\", \"./sed_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "370aca9e-525c-442f-9e7a-1bb6cd629040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 001 [001/649 (0%)]\tLoss: 10.290546\tPerplexity: 29452.86\telapsed: 0.01 mins\n",
      "Train Epoch: 001 [101/649 (16%)]\tLoss: 6.539432\tPerplexity: 691.89\telapsed: 1.02 mins\n",
      "Train Epoch: 001 [201/649 (31%)]\tLoss: 6.783267\tPerplexity: 882.95\telapsed: 2.13 mins\n",
      "Train Epoch: 001 [301/649 (46%)]\tLoss: 6.689163\tPerplexity: 803.65\telapsed: 3.30 mins\n",
      "Train Epoch: 001 [401/649 (62%)]\tLoss: 7.328592\tPerplexity: 1523.24\telapsed: 4.09 mins\n",
      "Train Epoch: 001 [501/649 (77%)]\tLoss: 6.189862\tPerplexity: 487.78\telapsed: 4.74 mins\n",
      "Train Epoch: 001 [601/649 (93%)]\tLoss: 6.760697\tPerplexity: 863.24\telapsed: 5.60 mins\n",
      "Train Epoch: 001 [649/649 (100%)]\tLoss: 7.102780\tPerplexity: 1215.34\telapsed: 6.14 mins\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15003/843170338.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15003/2037435627.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, val_dataloader, epochs, verbose, criterion, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_val_epoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maccum_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15003/2037435627.py\u001b[0m in \u001b[0;36m_val_epoch_step\u001b[0;34m(self, val_dataloader, criterion)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mword_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'test_loader'"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, val_dataloader, epochs=10, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "93cf6e6b-d42d-49cd-a30d-7ec632b4f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29345])\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"jumapili\"\n",
    "encoder_sentence = f.forEach(word_tokenizer.encode)(split_by_space)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inp_seq = torch.tensor(encoder_sentence(sample_sentence)).unsqueeze(dim=0)\n",
    "    out = model(inp_seq)\n",
    "    out = F.softmax(out, dim=1)\n",
    "\n",
    "    print(out.shape)    \n",
    "    out = torch.argmax(out)\n",
    "    word = word_tokenizer.decode(out.item())\n",
    "    \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "47055116-14ff-4ce6-9935-09778efe6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41515/41515 [00:00<00:00, 42115.63it/s]\n"
     ]
    }
   ],
   "source": [
    "word_seqs = get_better_sequences_from_txt_file(\"../resources/nelson/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "89d2519d-7e0a-459e-89f4-e049719e58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "all_words_iter = chain.from_iterable(word_seqs)\n",
    "words_occur_last_iter = tuple(word_seq[-1] for word_seq in word_seqs)\n",
    "\n",
    "main_counter = Counter(all_words_iter)\n",
    "last_token_occurance_counter = Counter(words_occur_last_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a68ced-5f7f-44ed-bd0c-ff2f038ebf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count = sum(map(lambda x: x[1], last_token_occurance_counter.items()))\n",
    "\n",
    "print(\"Token\\t|\\t%\")\n",
    "print(\"-\"*25)\n",
    "for i, c in last_token_occurance_counter.most_common(8):\n",
    "    print(\"{}\\t|\\t{:.06f}\".format(i, c / total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15962d-e54e-4048-b673-87011104df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token_occurance_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fb0ce-1fb4-4991-a17d-e31272f2bb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d387e5-11f4-4d95-a681-a07c16f4e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10565d4e-74e8-40f3-848c-6c790ec9ebe8",
   "metadata": {},
   "source": [
    "## Building utilities for marynlp\n",
    "\n",
    "Creating multiple utilities for building models, downloading the files and a tone of other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7766d9ea-6a86-487d-814c-5755f3ae6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module downloading sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7699651-5b78-4bc9-83db-0a8be81a41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the SED Morpheme template\n",
    "from marynlp.utils.storage import download as dl\n",
    "from marynlp.utils import storage\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# setup the download bucket\n",
    "bucket = storage.get_bucket(\"../resources/mary_africa_credentials_key.json\", \"marynlp-private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5c91c6-cdd7-4849-9f9a-7ef223ffe473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading th econtents needed\n",
    "\n",
    "def get_file_infer_google_cloud(blob_name, bucket):\n",
    "    _file = storage.local.get_path_from_store(blob_name)\n",
    "    \n",
    "    if not Path(_file).exists():\n",
    "        _file = dl.file_from_google_to_store(blob_name, bucket)\n",
    "\n",
    "    return str(_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f512ead-86d5-4cbc-891e-382a497db6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wa', 'lis', 'ma')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experimental.sed import MorphologyAnalyzer\n",
    "from typing import List, Tuple\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class WordBreaker(object):\n",
    "    def __init__(self, ma: MorphologyAnalyzer):\n",
    "        self.ma_ = ma\n",
    "        \n",
    "    def break_word(self, word: str) -> Tuple[str]:\n",
    "        return tuple([ su for su in self.ma_.break_text([word])[word]])\n",
    "    \n",
    "morpheme_template_file = get_file_infer_google_cloud(\"models/sed_morpheme_template.txt\", bucket)\n",
    "analyzer = MorphologyAnalyzer(morpheme_template_file); analyzer\n",
    "bk = WordBreaker(analyzer)\n",
    "bk.break_word(\"walisema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3fa56-39f6-4c36-b63a-86fc17fbe9af",
   "metadata": {},
   "source": [
    "### Voice sample\n",
    "\n",
    "Making the model file work as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984cdc1c-2ba2-4887-a4c8-d2e11c3e693a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/iam-kevin/.marynlp/store/voice/mnm-early-6k-ep100-bc64'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are sure we downloaded the file and it's placed here\n",
    "voice_model_path = str(storage.local.get_path_from_store('voice/mnm-early-6k-ep100-bc64')); voice_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8070d734-8699-49e0-84f5-703c09d1d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from experimental.voice.text_encoders import CharacterEncoder, GreedyEncoder\n",
    "from experimental.voice.nn import SpeechRecognitionModel\n",
    "\n",
    "\n",
    "# SPEECH_RECOGNITION_MODELS = {\n",
    "#     \"mnm23-early\": \"voice/mnm-early-6k-ep100-bc64.zip\"\n",
    "# }\n",
    "\n",
    "\n",
    "class InferenceSpeechRecognitionV23(object):\n",
    "    \"\"\"The Speech Recognizer\n",
    "\n",
    "    Args:\n",
    "        speech_recogn_model: A Trained Speech Recognition model\n",
    "        char_list: List of all the characters to consider in the model\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 speech_recogn_model: SpeechRecognitionModel,\n",
    "                 character_encoder: CharacterEncoder,\n",
    "                 test_transformer: nn.Module):\n",
    "\n",
    "        self.speech_model = speech_recogn_model\n",
    "        self.greedy_encoder = GreedyEncoder(character_encoder)\n",
    "        self.test_transform = test_transformer\n",
    "\n",
    "    def recognize(self, spect_tensor: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes in an input data\n",
    "        \"\"\"\n",
    "        output = self.test_transform(spect_tensor).unsqueeze(1)\n",
    "        output = self.speech_model(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "\n",
    "        return self.greedy_encoder.decode_test(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9fd6951-5d4f-4b25-8981-11bf6cb3884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iam-kevin/.marynlp/store/voice/mnm-early-6k-ep100-bc64/mnm-early-6k-ep100-bc64/final_model\n"
     ]
    }
   ],
   "source": [
    "!ls {voice_model_path}/mnm-early-6k-ep100-bc64/final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bffd09c4-0aef-4369-9d1c-bed1cd690d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InferenceSpeechRecognitionV23 at 0x7f1a135b3150>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "char_summary_csv_file = f\"{voice_model_path}/mnm-early-6k-ep100-bc64/char_summary.csv\"\n",
    "model_full_path = f\"{voice_model_path}/mnm-early-6k-ep100-bc64/final_model\"\n",
    "with open(char_summary_csv_file, 'r') as csvfilebuffer:\n",
    "    reader = csv.DictReader(csvfilebuffer)\n",
    "    ls = [row[\"char\"] for row in reader]\n",
    "\n",
    "char_encoder = CharacterEncoder(data=ls)\n",
    "\n",
    "\n",
    "default_model_params = dict(\n",
    "    n_cnn_layers=3,\n",
    "    n_rnn_layers=5,\n",
    "    rnn_dim=512,\n",
    "    n_feats=128,\n",
    ")\n",
    "\n",
    "# def load_model_from_path(model_path):\n",
    "speech_model = SpeechRecognitionModel(n_class=char_encoder.count + 1, **default_model_params)\n",
    "speech_model.load_state_dict(torch.load(str(model_full_path), map_location=torch.device('cpu')))\n",
    "\n",
    "# `MelSpectrogram` was the same transformation that was used when training th emode.\n",
    "#  so its safe to say that the same should be used for inference\n",
    "\n",
    "# NOTE: keep in mind that this is NOT  the version of pytorch that was used in the model creation\n",
    "inference_speech_model = InferenceSpeechRecognitionV23(speech_model, char_encoder, MelSpectrogram()); inference_speech_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63d772-29d8-41e8-8363-5904ee58b4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

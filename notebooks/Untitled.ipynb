{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4c4e91-77bc-4aa4-9710-043c2e7e382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1a84e-8e9c-4b33-8356-d0648ad4d33c",
   "metadata": {},
   "source": [
    "## Data handling thingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b04f5ccf-35bb-490b-9790-4f888e20be2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cpu'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "003a15c5-f783-4673-816d-02cee54e121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iam-kevin/Projects/ml/packages/mary/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25787a8e-31e1-409b-8adc-ce035aae0b50",
   "metadata": {},
   "source": [
    "## Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c993b0-4361-4b19-89db-b5749f7720e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class token(str):\n",
    "    def __init__(self, inp_: str, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.o = inp_\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"t'{self.o}'\"\n",
    "    \n",
    "class word(token):\n",
    "    def __repr__(self):\n",
    "        return f\"w'{self.o}'\"\n",
    "    \n",
    "class morph(token):\n",
    "    def __repr__(self):\n",
    "        return f\"m'{self.o}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80de72d1-1455-4897-aa8f-58828326c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(t'something', w'lo', m'lo')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token(\"something\"), word(\"lo\"), morph('lo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0973cb6-8492-4684-b99f-06f8f34652f9",
   "metadata": {},
   "source": [
    "`Vocab` section. Building the vocabulary structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc5f662-c1b1-4bea-81ae-95a52288ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict, defaultdict\n",
    "from typing import List, Optional, Union, Iterable\n",
    "\n",
    "# Vocabulary special for words\n",
    "# -------------------\n",
    "class Vocab(object):\n",
    "    def __init__(self, tokens: Iterable[token]):\n",
    "        self._tokens = list(tokens)\n",
    "        \n",
    "        # MAYBE: this might be a problem\n",
    "        self._tokens.sort()\n",
    "\n",
    "    def has(self, token: token):\n",
    "        return token in self._tokens\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        return self._tokens\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tokens)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        if len(self) > 4:\n",
    "            return \", \".join(self._tokens[:2]) + \"..., \" + self._tokens[-1]\n",
    "        \n",
    "        return \", \".join(self._token)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Vocab([%s], len=%d)\" % (self.extra_repr(), len(self))\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, file_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc21208f-1146-4882-ba5d-11080d8202fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "class Encoder(object):\n",
    "    def __init__(self, items: Iterable[str]):\n",
    "        items = tuple(items)\n",
    "        self._encode_map = dict(zip(items, range(len(items))))\n",
    "        self._decode_map = { v: k for k, v in self._encode_map.items()}\n",
    "    \n",
    "    def encode(self, item: str):\n",
    "        if item not in self._encode_map:\n",
    "            raise KeyError(\"Item '%s' is not in items\" % item)\n",
    "            \n",
    "        return self._encode_map[item]\n",
    "    \n",
    "    def decode(self, ix: int):\n",
    "        if ix >= len(self):\n",
    "            raise ValueError(\"Index %d is missing\" % (ix))\n",
    "\n",
    "        return self._decode_map[ix]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._encode_map)\n",
    "    \n",
    "OOV_TOKEN = token('<UNK>')\n",
    "\n",
    "class TokenEncoder(Encoder):\n",
    "    def __init__(self, vocab: Vocab):\n",
    "        super().__init__(vocab.get_tokens())\n",
    "#         self.oov = {} if oov is None else {it:len(self.items)+i for i,it in enumerate(oov) if it not in self.items}\n",
    "#         self.ix = len(self.oov)\n",
    "#         self.encode_unk = encode_unk\n",
    "    def encode(self, item: token):# , enc_unk=True):\n",
    "        try:\n",
    "            return super().encode(item)\n",
    "        except KeyError:\n",
    "            return -1\n",
    "#         item = item[0] if isinstance(item, list) else item\n",
    "\n",
    "#         if not enc_unk or item in self.items:\n",
    "#             return int(self.items.index(item)) if item in self.items else int(len(self.items)) \n",
    "\n",
    "#         if item in self.oov.keys():\n",
    "#             return self.oov[item]\n",
    "        \n",
    "#         if self.encode_unk:\n",
    "#             self.oov.update({item:int(len(self.items))+self.ix})\n",
    "#             self.ix+=1\n",
    "\n",
    "#             return self.oov[item]\n",
    "        \n",
    "#         return int(len(self.items))\n",
    "\n",
    "#     def encode_oov(self, oov):\n",
    "#         self.oov.update({it:len(self.items)+i for i,it in enumerate(oov) if it not in self.items})\n",
    "\n",
    "\n",
    "    def decode(self, ix):\n",
    "        try:\n",
    "            return super().decode(ix)\n",
    "        except ValueError:\n",
    "            return OOV_TOKEN\n",
    "#         if ix<len(self.items):\n",
    "#             return self.items[int(ix)]\n",
    "        \n",
    "#         return [(k,v) for k,v in self.oov.items() if v==ix][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee257a83-f8b5-4754-b6c5-c4dde740528a",
   "metadata": {},
   "source": [
    "## Data (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c314bea-140d-4b25-a5a2-decad6b23a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Iterable\n",
    "\n",
    "class MorphemeDataset(Dataset):\n",
    "    def __init__(self, word_morph_list: Iterable[List[str]]):\n",
    "        self.wml = list(word_morph_list)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        return np.array(self.wml[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.wml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7eaced-874f-455f-b699-2fe19cfe523b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eae645-feb5-41ef-b914-2160e7fb26f0",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "223aea1b-74c9-43a5-bd05-c9190c576c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "\n",
    "class SEDWordEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 token_vocab_size: int, \n",
    "                 embedding_dim: int, \n",
    "                 hidden_dim: int, \n",
    "                 dropout: int = 0.4, \n",
    "                 num_attn_layers=None, \n",
    "                 d_ff=None, \n",
    "                 hidden=None, \n",
    "                 out_c=None,\n",
    "                 kernel=None,\n",
    "                 padding_idx: int = 0,\n",
    "                 composition_fn='rnn'):\n",
    "        super(SEDWordEmbeddingLayer, self).__init__()\n",
    "        \n",
    "        self.comp_fn = composition_fn\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emb_mod = nn.Embedding(token_vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.compose, self.comp_linear = self.select_comp(dropout, num_attn_layers, d_ff, hidden, out_c, kernel)\n",
    "    \n",
    "    def select_comp(self, dropout, num_attn_layers, d_ff, hidden, out_c, kernel):\n",
    "        \"\"\"Helper to select composition function\"\"\"\n",
    "        compose = None\n",
    "        comp_linear = None\n",
    "        \n",
    "        if self.comp_fn == 'rnn':\n",
    "            compose = nn.GRU(input_size=self.embedding_dim, hidden_size=self.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)\n",
    "            comp_linear = nn.Sequential(\n",
    "                nn.Linear(self.hidden_dim*2, self.embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            \n",
    "        return compose, comp_linear\n",
    "\n",
    "    def padded_sum(self, data, input_len = None, dim=0):\n",
    "        \"\"\"\n",
    "        summing over padded sequence data\n",
    "        Args:\n",
    "            data: of dim (batch, seq_len, hidden_size)\n",
    "            input_lens: Optional long tensor of dim (batch,) that represents the\n",
    "                original lengths without padding. Tokens past these lengths will not\n",
    "                be included in the sum.\n",
    "\n",
    "        Returns:\n",
    "            Tensor (batch, hidden_size)\n",
    "\n",
    "        \"\"\"\n",
    "        if input_len is not None:\n",
    "            return torch.stack([\n",
    "                torch.sum(data[:, :input_len, :], dim=dim)\n",
    "            ])\n",
    "        else:\n",
    "            return torch.stack([torch.sum(data, dim=dim)])\n",
    "\n",
    "    def rnn_compose(self, emb_in):\n",
    "        \"\"\"\n",
    "        RNN composition of morpheme vectors into word embeddings\n",
    "        \"\"\"\n",
    "                           \n",
    "        return self.compose(emb_in)[0]\n",
    "    \n",
    "    def get_composition(self, emb_in, in_len, dim):\n",
    "        \"\"\"\n",
    "        Helper function to get word embeddings from morpheme vectors. Uses additive function by default\n",
    "        if composition function is not specified\n",
    "        \"\"\"\n",
    "        if self.comp_fn == 'rnn':\n",
    "            if len(in_len)>1 or (len(in_len)==1 and in_len[0] is not None):\n",
    "\n",
    "                emb_in = pack_padded_sequence(emb_in, in_len, batch_first=True, enforce_sorted=False)\n",
    "                rnn_out,_ = pad_packed_sequence(self.rnn_compose(emb_in), batch_first=True)\n",
    "\n",
    "            else:\n",
    "                rnn_out = self.rnn_compose(emb_in)[0].unsqueeze(0)\n",
    "\n",
    "            # if self.return_array:\n",
    "\n",
    "            #     return torch.mean(self.comp_linear(rnn_out),1).clone().detach().cpu().numpy()\n",
    "\n",
    "            return torch.mean(self.comp_linear(rnn_out),1).squeeze(0).clone().detach().cpu()\n",
    "\n",
    "        return self.padded_sum(emb_in, in_len, dim=dim)\n",
    "    \n",
    "    \n",
    "    def check_batched(self, inputs):\n",
    "        \"\"\"\n",
    "        check whether data is passed in batches(for models like the language and sentiment analysis)\n",
    "        or as single inputs and get embeddings accordingly.\n",
    "\n",
    "        Args:\n",
    "            inputs - collection containing the label-encoded words as well as their corresponding original\n",
    "                     lengths if were padded\n",
    "\n",
    "        Returns:\n",
    "            tensor of the vector representation of the input sequence\n",
    "        \"\"\"\n",
    "        emb_in = [self.forward(x_in, in_len) for x_in, in_len in zip(inputs[0], inputs[1])]\n",
    "\n",
    "        if self.comp_fn is not None:\n",
    "            emb_len = [torch.as_tensor(len(emb)) for emb in emb_in]\n",
    "            pad_in = pad_sequence(emb_in, batch_first=True)\n",
    "            packed_out = pack_padded_sequence(pad_in, emb_len, batch_first=True, enforce_sorted=False)\n",
    "            \n",
    "            return packed_out\n",
    "        \n",
    "        return torch.cat(emb_in, dim=0)\n",
    "\n",
    "    def forward(self, x_in, in_len, dim=1):\n",
    "        \"\"\"\n",
    "        Get embeddings from morpheme vectors after passing label-encoded vectors through the embedding layer\n",
    "        Args:\n",
    "            x_in   - label-encoded vector inputs\n",
    "            in_len - original lengths of vectors if were padded, else is None\n",
    "\n",
    "        Returns:\n",
    "            vector representation (embeddings) of the text sequence \n",
    "        \"\"\"  \n",
    "#         x_in, in_len = self.check_input(x_in, in_len)\n",
    "        emb_in = torch.cat([torch.stack([self.emb_mod(x)]) for x in x_in])\n",
    "        \n",
    "        return self.get_composition(emb_in, in_len, dim)\n",
    "\n",
    "# Handy functions\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def morpheme_pad_collate(batch):\n",
    "    x, x_len = [], []\n",
    "    \n",
    "    x_len = [m.shape[0] for m in batch]\n",
    "    max_len = max(x_len)\n",
    "    \n",
    "    x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "    xx = torch.stack([torch.from_numpy(x.reshape(1,-1)).long() for x in x_batch_])\n",
    "\n",
    "    return xx, torch.as_tensor(x_len).unsqueeze(dim=1)\n",
    "\n",
    "def build(token_vocab_size, embedding_dim, hidden_dim):\n",
    "    pass\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "class SEDWordEmbeddings(nn.Module):\n",
    "    def __init__(self, \n",
    "                 morpheme_vocab_size: int, \n",
    "                 embedding_dim: int, \n",
    "                 hidden_dim: int,\n",
    "                 use_cuda: bool = True\n",
    "                ):\n",
    "        super(SEDWordEmbeddings, self).__init__()\n",
    "        self.swe_layer = SEDWordEmbeddingLayer(morpheme_vocab_size, embedding_dim, hidden_dim)\n",
    "        self.swe_layer.eval() # freeze\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # device to train\n",
    "        self.device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        \n",
    "    def embed(self, t_morphs_of_word: torch.Tensor):\n",
    "        return self.swe_layer(t_morphs_of_word, [None])\n",
    "\n",
    "    def embed_morphemes(self, morphemes_word_list: Iterable[List[int]]) -> torch.Tensor: \n",
    "#         str_enc = torch.as_tensor([0])\n",
    "        \n",
    "#         tok, tok_len, str_enc = lm_pad_collate([(morpheme_indices, str_enc[0])])\n",
    "#         tok, tok_len = lm_pad_collate([morphemes_word_list])\n",
    "        \n",
    "        tok, tok_len = morphemes_word_list\n",
    "       \n",
    "        return pad_packed_sequence(\n",
    "            self.swe_layer.check_batched(\n",
    "                (tok, tok_len)\n",
    "            ), batch_first=True)[0].squeeze().clone().detach().cpu().numpy()\n",
    "    \n",
    "    def fit(self, dataset: MorphemeDataset, batch_size: int = 256):\n",
    "        dl = DataLoader(dataset, batch_size=batch_size, collate_fn=morpheme_pad_collate)\n",
    "        \n",
    "        _tok_emb = [self.embed_morphemes(d) for d in tqdm(dl)]\n",
    "        \n",
    "        # returns the embeddings\n",
    "        embeddings = np.concatenate(_tok_emb)\n",
    "        return embeddings\n",
    "\n",
    "#         self.embeddings = embeddings\n",
    "#         self.compose_embeddings.eval()\n",
    "\n",
    "#         if emb_store_path is not None or weight_store_path is not None:\n",
    "#             self.save_embeddings(weight_store_path, emb_store_path)\n",
    "            \n",
    "\n",
    "#     def save_to_path(self, embed_path: str):\n",
    "#         pass\n",
    "    \n",
    "#     @classmethod\n",
    "#     def load_from(self, embed_path: str):\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aae476-74dc-478e-8a54-88a62d90ebe1",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16dfb9dc-6557-4868-adbd-75f7617028bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class SEDLookupLayer(nn.Module):\n",
    "    def __init__(self, word_count: int, embedding_dim: int, comp_fn: str = None, rnn_dim: int = 32):\n",
    "        super(SEDLookupLayer, self).__init__()\n",
    "        self.comp_fn = comp_fn\n",
    "        self.birnn = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=rnn_dim, \n",
    "            num_layers=1, \n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, embedding_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            )\n",
    "        self.classifier = nn.Linear(embedding_dim, word_count)\n",
    "\n",
    "    def forward(self, emb_in):    \n",
    "#         output, _ = self.birnn(emb_in) if self.comp_fn is None else pad_packed_sequence(self.birnn(emb_in)[0], batch_first=True)\n",
    "        output, _ = self.birnn(emb_in)\n",
    "        output = self.linear(torch.mean(output, 1))  \n",
    "        \n",
    "        return self.classifier(output)\n",
    "        \n",
    "class SEDLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sed_word_embeddings_layer: SEDWordEmbeddingLayer,\n",
    "                 word_count: int, \n",
    "                 rnn_dim: int = 32):\n",
    "        super(SEDLanguageModel, self).__init__()\n",
    "        self.swe_layer = sed_word_embeddings_layer\n",
    "        self.look_up = SEDLookupLayer(word_count, self.swe_layer.embedding_dim, self.swe_layer.comp_fn, rnn_dim)\n",
    "#         self.birnn = nn.GRU(\n",
    "#             input_size=self.swe.embedding_dim, \n",
    "#             hidden_size=rnn_dim, \n",
    "#             num_layers=1, \n",
    "#             batch_first=True,\n",
    "#             bidirectional=True\n",
    "#         )\n",
    "#         self.linear = nn.Sequential(\n",
    "#             nn.Linear(rnn_dim*2, self.swe.embedding_dim), \n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             )\n",
    "#         self.classifier = nn.Linear(self.swe.embedding_dim, word_count)\n",
    "\n",
    "    def forward(self, inputs: List[Tuple[int]], input_len: List[int]):\n",
    "        emb_in = self.swe_layer(inputs, input_len)\n",
    "        \n",
    "        return self.look_up(emb_in.unsqueeze(dim=0))\n",
    "\n",
    "#         output,_ = self.birnn(emb_in) if self.swe.comp_fn is None else pad_packed_sequence(self.birnn(emb_in)[0], batch_first=True)\n",
    "#         output = self.linear(torch.mean(output, 1))  \n",
    "        \n",
    "#         return self.classifier(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f741c25-1549-4179-b6b7-bfedaf989a4a",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e6911-3769-455b-b572-d5930be1e1ed",
   "metadata": {},
   "source": [
    "## Building it all together\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9607011-b298-4a99-9f2c-c344f8b09bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing data source\n",
    "\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "# Data\n",
    "resources_folder = Path(\"../resources\")\n",
    "sample_file = resources_folder.joinpath(\"./train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a78f6496-0f4e-4c29-be80-ebca7047b19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../resources/build/df_morphs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14147/3787668457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmorpheme_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresources_folder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./build/df_morphs.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMorphologyAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmorpheme_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordBreaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/ml/packages/mary/experimental/sed/morphology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, morph_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmorph_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorph_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphemes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/ml/packages/mary/experimental/sed/morphology.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(self, store_path)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mload\u001b[0m \u001b[0mmorphemes\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mon\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         '''\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../resources/build/df_morphs.txt'"
     ]
    }
   ],
   "source": [
    "from experimental.sed import MorphologyAnalyzer\n",
    "# from marynlp import funcutils as f\n",
    "from typing import List, Tuple\n",
    "\n",
    "class WordBreaker(object):\n",
    "    def __init__(self, ma: MorphologyAnalyzer):\n",
    "        self.ma_ = ma\n",
    "        \n",
    "    def break_word(self, word: word) -> Tuple[morph]:\n",
    "        return tuple([ morph(su) for su in ma.break_text([word])[word]])\n",
    "        \n",
    "morpheme_path = resources_folder.joinpath(\"./build/df_morphs.txt\")\n",
    "ma = MorphologyAnalyzer(morph_path=morpheme_path)\n",
    "\n",
    "wb = WordBreaker(ma)\n",
    "\n",
    "# Sanity check\n",
    "wb.break_word(\"vita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f711164-3da8-45d8-90f8-56d3839e63c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../resources/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14147/351561511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0munique_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_words_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/ml/packages/mary/marynlp/funcutils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The output of fn '%s' is not iterable\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreader_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0miter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_skipper_exec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/ml/packages/mary/marynlp/funcutils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0miter_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/ml/packages/mary/marynlp/funcutils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0miter_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mo_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/ml/packages/mary/marynlp/funcutils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The output of fn '%s' is not iterable\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreader_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0miter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_skipper_exec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14147/351561511.py\u001b[0m in \u001b[0;36mget_unique_words_from_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforEach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_unique_words_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0munique_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_words_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14147/351561511.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../resources/train.txt'"
     ]
    }
   ],
   "source": [
    "from marynlp import funcutils as f\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import List, Union\n",
    "import os\n",
    "import re\n",
    "\n",
    "def split_by_space(text: str) -> List[str]:\n",
    "    \"\"\"Split a text to word strings\n",
    "    \n",
    "    Example\n",
    "    \"Lorem ipsum\" -> [ 'Lorem', 'ipsum' ] \n",
    "    \"\"\"\n",
    "    return re.split(r\"\\s+\", text)\n",
    "\n",
    "def read_file(file_path) -> List[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "@f.forEach(word, type_=set)\n",
    "@f.filterBy(lambda s: len(s.strip()) > 0)\n",
    "@f.flowBy(split_by_space)\n",
    "@f.forEach(lambda s: s.strip())\n",
    "def get_unique_words_from_file(file_path: os.PathLike):\n",
    "    return tqdm(read_file(file_path))\n",
    "\n",
    "unique_words = get_unique_words_from_file(sample_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e734aa23-a116-42fe-8fb7-62314ecc37f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14147/3584703577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmorp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmorp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0munique_morphemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unique_morphemes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_words' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_unique_morphemes(unique_words: Iterable[word], wb: WordBreaker) -> List[List[morph]]:\n",
    "    return set([m for morp in [wb.break_word(word) for word in tqdm(unique_words)] for m in morp])\n",
    "\n",
    "unique_morphemes = get_unique_morphemes(unique_words, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a251b803-8c05-4d86-8e0f-8311b0a83c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabularies for words and morphemes\n",
    "# -------\n",
    "\n",
    "word_vocab = Vocab(unique_words)\n",
    "morpheme_vocab = Vocab(unique_morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0643322d-2567-48a8-b9f0-95a77f47e0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "word_vocab.has(\"alikuja\"), word_vocab.has(\"asipokuja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b748a67-40b9-48ba-abd9-6fc5a10aee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_encoder = TokenEncoder(word_vocab)\n",
    "morpheme_encoder = TokenEncoder(morpheme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6fe23b-2279-42cb-b1ed-e7bd57fca1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(974, -1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "word_encoder.encode('alikuja'), word_encoder.encode('asipokuja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5df444a-e171-4927-8a8f-9944838e8631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, -1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAnity check\n",
    "morpheme_encoder.encode('ali'), morpheme_encoder.encode('zzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fada3d06-5eec-43b5-8179-993dd7618da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffcf9a9e229472a8cc5fc4c2adbaa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "# @f.apply(MorphemeDataset)\n",
    "def get_dataset_from_words(unique_words: Iterable[word], wb: WordBreaker) -> MorphemeDataset:\n",
    "    return ([ wb.break_word(word) for word in tqdm(unique_words)])\n",
    "\n",
    "morphs_word_list = get_dataset_from_words(word_vocab.get_tokens(), wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dd1168b-2f99-4ecc-b5bc-05edc1f84f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(m'aa', m'mbi', m'wa') [   4 2608 4761] (3,)\n",
      "(m'a', m'b', m'b', m'a', m's') [   3  387  387    3 3883] (5,)\n",
      "(m'a', m'ae') [ 3 47] (2,)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "md = MorphemeDataset([tuple([morpheme_encoder.encode(m) for m in morph]) for morph in morphs_word_list ])\n",
    "dl = DataLoader(md, batch_size=256, collate_fn=morpheme_pad_collate)\n",
    "\n",
    "# index = 6\n",
    "\n",
    "for index in [6, 19, 8]:\n",
    "    t = md[index]\n",
    "    word= morphs_word_list[index]\n",
    "    print(word, t, t.shape)\n",
    "# max_len = 5\n",
    "# np.pad(t, (0, max_len-t.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4e61cf37-b4db-4517-b34d-f0d91e1fc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = SEDLookupLayer(word_vocab.size, embedding_dim=100, comp_fn=swe.swe_layer.comp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "023966ea-b0fa-4cce-a83f-19a67dad4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_pad_collate(batch):\n",
    "    x, x_len = [], []\n",
    "    \n",
    "    x_len = [m.shape[0] for m in batch]\n",
    "    max_len = max(x_len)\n",
    "    \n",
    "    x_batch_ = [np.pad(t, (0, max_len - t.shape[0])) if t.shape[0] < max_len else t for t in batch]\n",
    "    xx = torch.stack([torch.from_numpy(x).long() for x in x_batch_])\n",
    "\n",
    "    return xx, torch.as_tensor(x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "519f4f7e-7d39-46b6-b299-b1f3243a9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = SEDWordEmbeddings(\n",
    "    morpheme_vocab.size,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=32\n",
    ")\n",
    "swl = SEDLanguageModel(swe.swe_layer, word_count=word_vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b54d3695-0c12-4ec3-acb4-15bd7ba140d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity fitting\n",
    "# --------------------------\n",
    "# embeddings = swe.fit(md)\n",
    "# embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e16e06bb-684b-4f75-a395-2d95a56d2f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1330,  0.0159, -0.0515,  ...,  0.0107,  0.0908, -0.0821]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "w'wanaoomba'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marynlp import funcutils as f\n",
    "import numpy as np\n",
    "\n",
    "# deal with word\n",
    "sentence = \"walimu wanasema tusome\"\n",
    "\n",
    "fn_get_tokenize = f.forEach(morpheme_encoder.encode)(wb.break_word)\n",
    "\n",
    "@f.forEach(fn_get_tokenize)\n",
    "def split_by_space(text: str):\n",
    "    return text.split(\" \")\n",
    "\n",
    "out = wb.break_word(\"ataenda\")\n",
    "array_split_by_space = f.forEach(np.array)(split_by_space)\n",
    "\n",
    "x, xl = lm_pad_collate(array_split_by_space(sentence))\n",
    "x, xl\n",
    "\n",
    "cc = swl(x, xl)\n",
    "\n",
    "# emb_in = swe.swe_layer(x, xl)\n",
    "# o, _ = look_up.birnn(emb_in.unsqueeze(dim=0))\n",
    "# lo = look_up.linear(torch.mean(o, 1))\n",
    "\n",
    "\n",
    "# cc = look_up.classifier(lo)\n",
    "print(cc)\n",
    "am = torch.argmax(cc)\n",
    "word_encoder.decode(am.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98d3927a-e99d-4606-9156-43f2b847555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3158, 3345], [1354, 2313], [4497, -1]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from typing import Iterable, List\n",
    "from collections.abc import Callable\n",
    "\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 list_of_sentences: List[str],\n",
    "                 word_tokenize: Callable):\n",
    "        self.ls = list_of_sentences\n",
    "        self.word_tokenize = word_tokenize\n",
    "\n",
    "    def split_words(self, sentence: str) -> Iterable[str]:\n",
    "        return re.split(r\"\\s+\", sentence)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ls)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        return [self.word_tokenize(w) for w in self.split_words(self.ls[index])]\n",
    "\n",
    "    \n",
    "fn_get_tokenize = f.forEach(morpheme_encoder.encode)(wb.break_word)\n",
    "lm_dataset = LMDataset([\"walimu wanasema tusome\", \"njoo shule uimbe\"], fn_get_tokenize)\n",
    "lm_dataset[1]\n",
    "\n",
    "train_dataloader = DataLoader(lm_dataset, batch_size=3, collate_fn=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745e68c-ab38-4e0d-b6b9-68c8016fe591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to train state\n",
    "swl.train()\n",
    "        \n",
    "start_time = time()\n",
    "data_len = len(self.train_loader.dataset)\n",
    "\n",
    "for batch_idx, _data in enumerate(self.train_loader):\n",
    "    x, x_len, y = _data \n",
    "\n",
    "    y = [y_.to(ModelsConfig.device) for y_ in [y]]\n",
    "    y = [y_.reshape(y_.shape[0],) for y_ in y]\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    output = self.model((x, x_len))\n",
    "    loss = self.criterion(output, y[0])\n",
    "    loss.backward()\n",
    "\n",
    "    self.optimizer.step()\n",
    "    if batch_idx % verbose == 0 or batch_idx == data_len:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tPerplexity: {:5.2f}\\telapsed: {:.2f} mins'.format(\n",
    "            epoch, batch_idx * len(x), data_len,\n",
    "            100. * batch_idx / len(self.train_loader), loss.item(), np.exp(loss.item()), (time()-start_time)/60))#, loss2.item()))\\tap_Loss: {:.6f}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4055d1-0399-472a-a8e6-07c844e23cee",
   "metadata": {},
   "source": [
    "## Testing for word relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "15f83dc6-e4ac-48bb-b413-23e2181dcb55",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'word_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-e87386c4bc5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSEDLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'word_count'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class WordRelations(object):\n",
    "    def __init__(self, \n",
    "                 swe: SEDWordEmbeddings, \n",
    "                 embeddings: np.ndarray,\n",
    "                 word_encoder: TokenEncoder,\n",
    "                 morpheme_encoder: TokenEncoder,\n",
    "                 word_breaker: WordBreaker\n",
    "                ):\n",
    "        self.swe = swe\n",
    "        self.word_encoder = word_encoder\n",
    "        self.morpheme_encoder = morpheme_encoder\n",
    "        self.wb = word_breaker\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def get_word_embedding(self, word: str) -> torch.Tensor:\n",
    "        morphs_of_word = [ self.morpheme_encoder.encode(m) for m in self.wb.break_word(word)]\n",
    "        tmw = torch.tensor([morphs_of_word])\n",
    "\n",
    "        return self.swe.embed(tmw)\n",
    "\n",
    "    def get_most_similar(self, string: str, sim_dict: int, threshold: int):\n",
    "        \"\"\"\n",
    "        get most similar word(s) from collection of related words using the cosine similarity measure\n",
    "\n",
    "        Args:\n",
    "            string    - string whose most siilar words are to be obtained\n",
    "            sim_dict  - dictionary of similar words\n",
    "            threshold - minimum cosine similarity value for words to be considered most similar to string\n",
    "                        if None then only word with highest cosine similarity is returned\n",
    "        \n",
    "        Returns:\n",
    "            collection of most similar words as determined by their cosine similarity to the string being considered\n",
    "        \"\"\"\n",
    "\n",
    "        cos_sim = [sim[1] for sim in sim_dict[string]]\n",
    "        max_sim = max(cos_sim)\n",
    "\n",
    "        if threshold is not None:\n",
    "            assert max_sim>=threshold, 'threshold set too high, no similar words found'\n",
    "\n",
    "            return [v for v in sim_dict[string] if v[1]>=threshold]\n",
    "\n",
    "        return [v for v in sim_dict[string] if v[1]==max_sim]\n",
    "\n",
    "    def get_similar_words(self, string, k_dim=0, threshold=None):\n",
    "        \"\"\"\n",
    "        get collection of closely related words usnig the cosine similarity of their embedding vectors\n",
    "\n",
    "        Args:\n",
    "            string          - string whose related words are to be obtained\n",
    "            embeddings_dict - dictionary of word embeddings. If embedder already trained uses existing embeddings.\n",
    "            threshold       - minimum cosine similarity for word to be considered similar to given word\n",
    "\n",
    "        Returns:\n",
    "            dictionary of similar words and their similarity as measure by the cosine similarity between their embedding vectors\n",
    "            and that of the string\n",
    "        \"\"\"\n",
    "#         self.check_embeddings()\n",
    "        val = self.get_word_embedding(string)\n",
    "        \n",
    "        sim_dict = {}\n",
    "        sim_dict[string] = [(txt, cosine_similarity(val.reshape(1,-1), vec.reshape(1,-1)).reshape(1)[0]) for txt,vec in enumerate(self.embeddings) if txt!=string or not (vec==val).all()]\n",
    "        \n",
    "        most_similar = self.get_most_similar(string, sim_dict, threshold)\n",
    "        sim_dict[string] = sorted(most_similar,key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sim_dict\n",
    "\n",
    "    def get_best_analogy(self, sim_list, string_b, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        get most relevant analogy from collection of analogous words. uses cosine similarity measure to determine \n",
    "        the best analogy\n",
    "\n",
    "        Args:\n",
    "            sim_list - list of words similar to the given word\n",
    "            string_b - word whose analogy is to be determined\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "\n",
    "        Returns:\n",
    "            analogy of the given word\n",
    "        \"\"\"\n",
    "        sorted_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)\n",
    "        max_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)[0][0]\n",
    "        \n",
    "        if not return_cos_similarity:\n",
    "            sorted_sim = [sim[0] for sim in sorted_sim]\n",
    "\n",
    "        if max_sim == self.word_encoder.encode(string_b):\n",
    "            return self.word_encoder.decode(sorted_sim[1])\n",
    "        \n",
    "        return self.word_encoder.decode(sorted_sim[0])\n",
    "\n",
    "    def _3_cos_add(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = b - a + _a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1),_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "  \n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def _3_cos_mul(self, a, _a, b, string_b, k_dim, return_cos_similarity, eps=0.001):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on a multiplicative function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        \n",
    "        sim_list = [(txt, (cosine_similarity(vec.reshape(1,-1),b).reshape(1)[0]*cosine_similarity(vec.reshape(1,-1),_a).reshape(1)[0])/(cosine_similarity(vec.reshape(1,-1),a).reshape(1)[0]+eps))\\\n",
    "                    for txt,vec in enumerate(self.embeddings)]\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def pair_direction(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities that maintains\n",
    "        the ...\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = _a - a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1)-b,_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def get_analogy(self, string_a, analogy_a, string_b, k_dim=0, return_cos_similarity=False):\n",
    "        \"\"\"\n",
    "        get analogous words using 3COSADD, PAIRDIRECTION, or 3COSMUL which make use of the cosine similarity of the embedding vectors.        \n",
    "        adapted from: https://www.aclweb.org/anthology/W14-1618\n",
    "\n",
    "        Args:\n",
    "            string_a, analogy_a - example of a string and its analogy\n",
    "            string_b - string whose analogy is to be determined\n",
    "            embeddings_dict - dictionary of embeddings. uses existing embeddings if was pretrained\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        a, _a, b = (self.get_word_embedding(string).reshape(1,-1) for string in [string_a, analogy_a, string_b])\n",
    "        \n",
    "#         if self.compose_embeddings.comp_fn is None:\n",
    "#             return self._3_cos_add(a, _a, b, string_b, k_dim, return_cos_similarity)\n",
    "            \n",
    "        return self._3_cos_mul(a, _a, b, string_b, k_dim, return_cos_similarity) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ceeed22c-4565-476d-b032-d4efe78c15e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w'tabasamu'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this might take a minute\n",
    "wrl = WordRelations(swe, embeddings, word_encoder, morpheme_encoder, wb)\n",
    "wrl.get_analogy(\"ataenda\", \"alienda\", \"atakimbia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe406b-b28d-45e7-b82f-c95b2a9a83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alienda\n"
     ]
    }
   ],
   "source": [
    "# this.... minutes\n",
    "for word in ['ataenda', 'atacheza', 'ataanza', 'atabadilika']:    \n",
    "    print(wrl.get_analogy('ataondoka', 'aliondoka', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1055059-ea75-4b8d-811f-3ab5f53bd865",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db947bc-907e-4ff0-9040-da62d7fe67f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

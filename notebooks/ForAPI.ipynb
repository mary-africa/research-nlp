{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579133ac-2e12-4201-a115-3fc7db44af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5387d344-fbb4-495d-92b9-b33322107d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'marynlp.text.processors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37870/574090888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarynlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_punctuations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhite_space_cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarynlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuncutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'marynlp.text.processors'"
     ]
    }
   ],
   "source": [
    "## Setting up the needed to do anything\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Iterable\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def ignore_rules(text: str):\n",
    "    return not (text.find(\"<text\") > -1 or text.find(\"</text>\") > -1)\n",
    "\n",
    "def should_be_longer_that_20(text: str):\n",
    "    return len(text) > 20\n",
    "\n",
    "@f.forEach(lowercase)\n",
    "@f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))\n",
    "def load_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.readlines()\n",
    "    \n",
    "def save_to_file(file, content: Iterable[str]):\n",
    "    with open(file, \"w\") as wf:\n",
    "        for line in content:\n",
    "            wf.write(line)\n",
    "    \n",
    "filtered_fn = f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))(load_file)\n",
    "\n",
    "folder_path = \"../resources/operate_on\"\n",
    "!mkdir -p {folder_path}\n",
    "save_to_file(f'{folder_path}/dummy.txt', filtered_fn(sample_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17faa09c-2010-4225-8ab6-29245ee4a11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/iam-kevin/.marynlp/store/models/sed_morpheme_template.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the bucket to load the data\n",
    "# -----------------------------------------\n",
    "\n",
    "## Using the SED Morpheme template\n",
    "from marynlp.utils.storage import download as dl\n",
    "from marynlp.utils import storage\n",
    "\n",
    "# setup the download bucket\n",
    "bucket = storage.get_bucket(\"../resources/mary_africa_credentials_key.json\", \"marynlp-private\")\n",
    "morpheme_template_file = dl.file_from_google_to_store(\"models/sed_morpheme_template.txt\", bucket); morpheme_template_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68279-5768-4079-ac7b-3c69da64669b",
   "metadata": {},
   "source": [
    "## Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2761eae-0ec8-4ec8-964c-1b7b0ef490e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nili', 'e', 'nda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experimental.sed import MorphologyAnalyzer\n",
    "from typing import List, Tuple\n",
    "\n",
    "analyzer = MorphologyAnalyzer(morpheme_template_file); analyzer\n",
    "\n",
    "def break_word(word: str) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "# /text/tokenize[?type=sed] (default: sed)\n",
    "break_word(\"nilienda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3b8dd-cb6c-4bfb-827d-eb6dbeee3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class WordRelations(object):\n",
    "    def __init__(self, \n",
    "                 swe: SEDWordEmbeddings, \n",
    "                 embeddings: np.ndarray,\n",
    "                 word_encoder: TokenEncoder,\n",
    "                 morpheme_encoder: TokenEncoder,\n",
    "                 word_breaker: WordBreaker\n",
    "                ):\n",
    "        self.swe = swe\n",
    "        self.word_encoder = word_encoder\n",
    "        self.morpheme_encoder = morpheme_encoder\n",
    "        self.wb = word_breaker\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def get_word_embedding(self, word: str) -> torch.Tensor:\n",
    "        morphs_of_word = [ self.morpheme_encoder.encode(m) for m in self.wb.break_word(word)]\n",
    "        tmw = torch.tensor([morphs_of_word])\n",
    "\n",
    "        return self.swe.embed(tmw)\n",
    "\n",
    "    def get_most_similar(self, string: str, sim_dict: int, threshold: int):\n",
    "        \"\"\"\n",
    "        get most similar word(s) from collection of related words using the cosine similarity measure\n",
    "\n",
    "        Args:\n",
    "            string    - string whose most siilar words are to be obtained\n",
    "            sim_dict  - dictionary of similar words\n",
    "            threshold - minimum cosine similarity value for words to be considered most similar to string\n",
    "                        if None then only word with highest cosine similarity is returned\n",
    "        \n",
    "        Returns:\n",
    "            collection of most similar words as determined by their cosine similarity to the string being considered\n",
    "        \"\"\"\n",
    "\n",
    "        cos_sim = [sim[1] for sim in sim_dict[string]]\n",
    "        max_sim = max(cos_sim)\n",
    "\n",
    "        if threshold is not None:\n",
    "            assert max_sim>=threshold, 'threshold set too high, no similar words found'\n",
    "\n",
    "            return [v for v in sim_dict[string] if v[1]>=threshold]\n",
    "\n",
    "        return [v for v in sim_dict[string] if v[1]==max_sim]\n",
    "\n",
    "    def get_similar_words(self, string, k_dim=0, threshold=None):\n",
    "        \"\"\"\n",
    "        get collection of closely related words usnig the cosine similarity of their embedding vectors\n",
    "\n",
    "        Args:\n",
    "            string          - string whose related words are to be obtained\n",
    "            embeddings_dict - dictionary of word embeddings. If embedder already trained uses existing embeddings.\n",
    "            threshold       - minimum cosine similarity for word to be considered similar to given word\n",
    "\n",
    "        Returns:\n",
    "            dictionary of similar words and their similarity as measure by the cosine similarity between their embedding vectors\n",
    "            and that of the string\n",
    "        \"\"\"\n",
    "#         self.check_embeddings()\n",
    "        val = self.get_word_embedding(string)\n",
    "        \n",
    "        sim_dict = {}\n",
    "        sim_dict[string] = [(txt, cosine_similarity(val.reshape(1,-1), vec.reshape(1,-1)).reshape(1)[0]) for txt,vec in enumerate(self.embeddings) if txt!=string or not (vec==val).all()]\n",
    "        \n",
    "        most_similar = self.get_most_similar(string, sim_dict, threshold)\n",
    "        sim_dict[string] = sorted(most_similar,key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sim_dict\n",
    "\n",
    "    def get_best_analogy(self, sim_list, string_b, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        get most relevant analogy from collection of analogous words. uses cosine similarity measure to determine \n",
    "        the best analogy\n",
    "\n",
    "        Args:\n",
    "            sim_list - list of words similar to the given word\n",
    "            string_b - word whose analogy is to be determined\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "\n",
    "        Returns:\n",
    "            analogy of the given word\n",
    "        \"\"\"\n",
    "        sorted_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)\n",
    "        max_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)[0][0]\n",
    "        \n",
    "        if not return_cos_similarity:\n",
    "            sorted_sim = [sim[0] for sim in sorted_sim]\n",
    "\n",
    "        if max_sim == self.word_encoder.encode(string_b):\n",
    "            return self.word_encoder.decode(sorted_sim[1])\n",
    "        \n",
    "        return self.word_encoder.decode(sorted_sim[0])\n",
    "\n",
    "    def _3_cos_add(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = b - a + _a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1),_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "  \n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def _3_cos_mul(self, a, _a, b, string_b, k_dim, return_cos_similarity, eps=0.001):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on a multiplicative function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        \n",
    "        sim_list = [(txt, (cosine_similarity(vec.reshape(1,-1),b).reshape(1)[0]*cosine_similarity(vec.reshape(1,-1),_a).reshape(1)[0])/(cosine_similarity(vec.reshape(1,-1),a).reshape(1)[0]+eps))\\\n",
    "                    for txt,vec in enumerate(self.embeddings)]\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def pair_direction(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities that maintains\n",
    "        the ...\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = _a - a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1)-b,_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def get_analogy(self, string_a, analogy_a, string_b, k_dim=0, return_cos_similarity=False):\n",
    "        \"\"\"\n",
    "        get analogous words using 3COSADD, PAIRDIRECTION, or 3COSMUL which make use of the cosine similarity of the embedding vectors.        \n",
    "        adapted from: https://www.aclweb.org/anthology/W14-1618\n",
    "\n",
    "        Args:\n",
    "            string_a, analogy_a - example of a string and its analogy\n",
    "            string_b - string whose analogy is to be determined\n",
    "            embeddings_dict - dictionary of embeddings. uses existing embeddings if was pretrained\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        a, _a, b = (self.get_word_embedding(string).reshape(1,-1) for string in [string_a, analogy_a, string_b])\n",
    "        \n",
    "#         if self.compose_embeddings.comp_fn is None:\n",
    "#             return self._3_cos_add(a, _a, b, string_b, k_dim, return_cos_similarity)\n",
    "            \n",
    "        return self._3_cos_mul(a, _a, b, string_b, k_dim, return_cos_similarity) \n",
    "\n",
    "    \n",
    "# swe = \n",
    "# /text/analogies?a=wanacheza&b=atacheza[&type=sed] (default: sed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4bb2c-1d1b-49f5-83e2-0e710afd4c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2a9fb5-c93e-48cf-bbaa-7a215de7dc3b",
   "metadata": {},
   "source": [
    "## LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66d7b6c-d6ff-4eb8-8250-f06eaf5f1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Iterable, List\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "@f.apply(set)\n",
    "def get_unique_item_from_item_sequences(word_sequences: Iterable[Iterable[str]]):\n",
    "    for word_seq in word_sequences:\n",
    "        for word in word_seq:\n",
    "            word = word.strip()\n",
    "            if len(word) != 0:\n",
    "                yield word\n",
    "\n",
    "@f.apply(set)\n",
    "def get_unique_morphemes(words: Iterable[str], analyzer):\n",
    "    analyzer_break_word = (f.partial(break_word, analyzer=analyzer))\n",
    "\n",
    "    for word in tqdm(words):\n",
    "        for morph in analyzer_break_word(word):\n",
    "            yield morph\n",
    "\n",
    "\n",
    "\n",
    "def read_text_from_files(txt_file_path: str):\n",
    "    with open(txt_file_path, mode=\"r\") as rb:\n",
    "        return rb.readlines()\n",
    "\n",
    "import re\n",
    "read_word_sequence_from_file = f.forEach(lambda l: re.split(r\"\\s+\", l.strip()))(read_text_from_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b956c6bd-58e5-4ae9-9de8-bd56578a62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from marynlp.text.processor import TokenEncoder\n",
    "from marynlp.text.data import Vocab\n",
    "\n",
    "from experimental.sed import MorphologyAnalyzer\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def break_word(word: str, analyzer) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "@f.apply(set)\n",
    "def load_tokens(store_path):\n",
    "    with open(str(store_path), 'r') as file:\n",
    "        for y in json.load(file):\n",
    "            for i in y:\n",
    "                yield i\n",
    "\n",
    "analyzer = MorphologyAnalyzer(\"../resources/pretrained/models_sed_morpheme_template.txt\")\n",
    "unique_morphemes = load_tokens(\"../resources/pretrained/models_small-tkns.txt\")\n",
    "morpheme_tokenizer = TokenEncoder(Vocab(unique_morphemes))\n",
    "unique_words = get_unique_item_from_item_sequences(read_word_sequence_from_file(\"../resources/nelson/train.txt\"))\n",
    "\n",
    "\n",
    "encode_word = f.forEach(morpheme_tokenizer.encode)(f.partial(break_word, analyzer=analyzer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9e4048-6905-4ba5-aba4-c82a6d3bcc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marynlp.text.processor import BaseTokenEncoder \n",
    "unique_words = get_unique_item_from_item_sequences(read_word_sequence_from_file(\"../resources/nelson/train.txt\"))\n",
    "\n",
    "# Adding the unknown marker at the end\n",
    "# This would compensate for the LabelEncoder\n",
    "word_tokenizer = BaseTokenEncoder(Vocab(unique_words).get_tokens() + ['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439544eb-c5a3-4409-8970-e9d0d4c8a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.sed.modules import CompositionalEmbeddings, CompositionalLanguageModel\n",
    "import torch\n",
    "\n",
    "com_emb = CompositionalEmbeddings(\n",
    "    morpheme_vocab_size=morpheme_tokenizer.size,\n",
    "    embedding_dim = 512,\n",
    "    hidden_dim = 512,\n",
    "    composition_fn = 'rnn',\n",
    "    use_cuda=False\n",
    ");\n",
    "com_emb.update_weight_from_path(\"../resources/pretrained/models_embeddings_small-embeddings.pth\")\n",
    "\n",
    "com_lm = CompositionalLanguageModel(com_emb, \n",
    "                           word_tokenizer.size,\n",
    "                           rnn_dim = 512,\n",
    "                            use_cuda=False)\n",
    "com_lm.update_weight_from_path(\"../resources/pretrained/models_language-model_6.4819_lm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bcfa1c03-3d4d-45a0-92af-d0bcbcbfaf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o1 = com_emb.compose_embeddings.emb_mod(torch.tensor(1))\n",
    "# o2 = com_lm.morph_embedder.compose_embeddings.emb_mod(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "342782e9-40f2-445f-9098-4cdea6a04012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text = \"mamba\"\n",
    "words = text.split(); words\n",
    "\n",
    "def encode_sentence(sentence: str, encode_word_to_morphs):\n",
    "    words = sentence.split()\n",
    "    return pad_sequence(list(map(lambda b: torch.tensor(encode_word_to_morphs(b)), words)), batch_first=True)\n",
    "\n",
    "def next_token_predict_proba(input_tensor: torch.Tensor, language_model: nn.Module):\n",
    "    with torch.no_grad():\n",
    "        language_model.eval()\n",
    "        output = language_model(vl)\n",
    "        return F.softmax(output, dim=1)\n",
    "\n",
    "# o = F.softmax(output, dim=1)\n",
    "# word_ix = torch.argmax(o).item()\n",
    "# word_tokenizer.decode(word_ix),words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d8a456d-2d27-4f2e-b35c-67cd8c3d6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_in_sentence = f.apply(\n",
    "                            f.calls(\n",
    "                                torch.argmax,\n",
    "                                lambda b: word_tokenizer.decode(b.item())\n",
    "                            ),\n",
    "                        )(f.apply(f.partial(next_token_predict_proba, language_model=com_lm))(f.partial(encode_sentence, encode_word_to_morphs=encode_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8dc75c20-046f-412a-83e8-8bc680ea1d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moja'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_in_sentence(\"kesho ninaenda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d330a-2451-4255-ae58-50475c50cbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

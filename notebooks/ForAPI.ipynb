{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579133ac-2e12-4201-a115-3fc7db44af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.insert(0,\"../\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5387d344-fbb4-495d-92b9-b33322107d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the needed to do anything\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Iterable\n",
    "import os\n",
    "\n",
    "from marynlp.text.processors.formatters import lowercase, remove_punctuations, white_space_cleaning\n",
    "from marynlp import funcutils as f\n",
    "\n",
    "data_path = Path(\"../resources/data\")\n",
    "helsinki_na_path = data_path / Path(\"./hcs-na-v2\")\n",
    "\n",
    "# File to test out the concept\n",
    "sample_file = helsinki_na_path / Path(\"./new-mat/bunge/han1-2004.shu\")\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def ignore_rules(text: str):\n",
    "    return not (text.find(\"<text\") > -1 or text.find(\"</text>\") > -1)\n",
    "\n",
    "def should_be_longer_that_20(text: str):\n",
    "    return len(text) > 20\n",
    "\n",
    "@f.forEach(lowercase)\n",
    "@f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))\n",
    "def load_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.readlines()\n",
    "    \n",
    "def save_to_file(file, content: Iterable[str]):\n",
    "    with open(file, \"w\") as wf:\n",
    "        for line in content:\n",
    "            wf.write(line)\n",
    "    \n",
    "filtered_fn = f.filterBy(f.rules(should_be_longer_that_20, ignore_rules))(load_file)\n",
    "\n",
    "folder_path = \"../resources/operate_on\"\n",
    "!mkdir -p {folder_path}\n",
    "save_to_file(f'{folder_path}/dummy.txt', filtered_fn(sample_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17faa09c-2010-4225-8ab6-29245ee4a11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/iam-kevin/.marynlp/store/models/sed_morpheme_template.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the bucket to load the data\n",
    "# -----------------------------------------\n",
    "\n",
    "## Using the SED Morpheme template\n",
    "from marynlp.utils.storage import download as dl\n",
    "from marynlp.utils import storage\n",
    "\n",
    "# setup the download bucket\n",
    "bucket = storage.get_bucket(\"../resources/mary_africa_credentials_key.json\", \"marynlp-private\")\n",
    "morpheme_template_file = dl.file_from_google_to_store(\"models/sed_morpheme_template.txt\", bucket); morpheme_template_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a68279-5768-4079-ac7b-3c69da64669b",
   "metadata": {},
   "source": [
    "## Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2761eae-0ec8-4ec8-964c-1b7b0ef490e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nili', 'e', 'nda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experimental.sed import MorphologyAnalyzer\n",
    "from typing import List, Tuple\n",
    "\n",
    "analyzer = MorphologyAnalyzer(morpheme_template_file); analyzer\n",
    "\n",
    "def break_word(word: str) -> Tuple[str]:\n",
    "    return tuple([ su for su in analyzer.break_text([word])[word]])\n",
    "\n",
    "# /text/tokenize[?type=sed] (default: sed)\n",
    "break_word(\"nilienda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3b8dd-cb6c-4bfb-827d-eb6dbeee3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class WordRelations(object):\n",
    "    def __init__(self, \n",
    "                 swe: SEDWordEmbeddings, \n",
    "                 embeddings: np.ndarray,\n",
    "                 word_encoder: TokenEncoder,\n",
    "                 morpheme_encoder: TokenEncoder,\n",
    "                 word_breaker: WordBreaker\n",
    "                ):\n",
    "        self.swe = swe\n",
    "        self.word_encoder = word_encoder\n",
    "        self.morpheme_encoder = morpheme_encoder\n",
    "        self.wb = word_breaker\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def get_word_embedding(self, word: str) -> torch.Tensor:\n",
    "        morphs_of_word = [ self.morpheme_encoder.encode(m) for m in self.wb.break_word(word)]\n",
    "        tmw = torch.tensor([morphs_of_word])\n",
    "\n",
    "        return self.swe.embed(tmw)\n",
    "\n",
    "    def get_most_similar(self, string: str, sim_dict: int, threshold: int):\n",
    "        \"\"\"\n",
    "        get most similar word(s) from collection of related words using the cosine similarity measure\n",
    "\n",
    "        Args:\n",
    "            string    - string whose most siilar words are to be obtained\n",
    "            sim_dict  - dictionary of similar words\n",
    "            threshold - minimum cosine similarity value for words to be considered most similar to string\n",
    "                        if None then only word with highest cosine similarity is returned\n",
    "        \n",
    "        Returns:\n",
    "            collection of most similar words as determined by their cosine similarity to the string being considered\n",
    "        \"\"\"\n",
    "\n",
    "        cos_sim = [sim[1] for sim in sim_dict[string]]\n",
    "        max_sim = max(cos_sim)\n",
    "\n",
    "        if threshold is not None:\n",
    "            assert max_sim>=threshold, 'threshold set too high, no similar words found'\n",
    "\n",
    "            return [v for v in sim_dict[string] if v[1]>=threshold]\n",
    "\n",
    "        return [v for v in sim_dict[string] if v[1]==max_sim]\n",
    "\n",
    "    def get_similar_words(self, string, k_dim=0, threshold=None):\n",
    "        \"\"\"\n",
    "        get collection of closely related words usnig the cosine similarity of their embedding vectors\n",
    "\n",
    "        Args:\n",
    "            string          - string whose related words are to be obtained\n",
    "            embeddings_dict - dictionary of word embeddings. If embedder already trained uses existing embeddings.\n",
    "            threshold       - minimum cosine similarity for word to be considered similar to given word\n",
    "\n",
    "        Returns:\n",
    "            dictionary of similar words and their similarity as measure by the cosine similarity between their embedding vectors\n",
    "            and that of the string\n",
    "        \"\"\"\n",
    "#         self.check_embeddings()\n",
    "        val = self.get_word_embedding(string)\n",
    "        \n",
    "        sim_dict = {}\n",
    "        sim_dict[string] = [(txt, cosine_similarity(val.reshape(1,-1), vec.reshape(1,-1)).reshape(1)[0]) for txt,vec in enumerate(self.embeddings) if txt!=string or not (vec==val).all()]\n",
    "        \n",
    "        most_similar = self.get_most_similar(string, sim_dict, threshold)\n",
    "        sim_dict[string] = sorted(most_similar,key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sim_dict\n",
    "\n",
    "    def get_best_analogy(self, sim_list, string_b, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        get most relevant analogy from collection of analogous words. uses cosine similarity measure to determine \n",
    "        the best analogy\n",
    "\n",
    "        Args:\n",
    "            sim_list - list of words similar to the given word\n",
    "            string_b - word whose analogy is to be determined\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "\n",
    "        Returns:\n",
    "            analogy of the given word\n",
    "        \"\"\"\n",
    "        sorted_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)\n",
    "        max_sim = sorted([sim for sim in sim_list if sim[1]>0], key=lambda x:x[1], reverse=True)[0][0]\n",
    "        \n",
    "        if not return_cos_similarity:\n",
    "            sorted_sim = [sim[0] for sim in sorted_sim]\n",
    "\n",
    "        if max_sim == self.word_encoder.encode(string_b):\n",
    "            return self.word_encoder.decode(sorted_sim[1])\n",
    "        \n",
    "        return self.word_encoder.decode(sorted_sim[0])\n",
    "\n",
    "    def _3_cos_add(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = b - a + _a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1),_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "  \n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def _3_cos_mul(self, a, _a, b, string_b, k_dim, return_cos_similarity, eps=0.001):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on a multiplicative function of cosine similarities\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of the string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        \n",
    "        sim_list = [(txt, (cosine_similarity(vec.reshape(1,-1),b).reshape(1)[0]*cosine_similarity(vec.reshape(1,-1),_a).reshape(1)[0])/(cosine_similarity(vec.reshape(1,-1),a).reshape(1)[0]+eps))\\\n",
    "                    for txt,vec in enumerate(self.embeddings)]\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def pair_direction(self, a, _a, b, string_b, k_dim, return_cos_similarity):\n",
    "        \"\"\"\n",
    "        determine the analogy of the given word based on an additive function of cosine similarities that maintains\n",
    "        the ...\n",
    "\n",
    "        Args:\n",
    "            a,_a     - vector representation of the example of a word and its corresponding analogy\n",
    "            b        - vecor representation of the string whose analogy is to be determined\n",
    "            string_b - string whose analogy is to be determined\n",
    "\n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        _b = _a - a\n",
    "\n",
    "        sim_list = [(txt, cosine_similarity(vec.reshape(1,-1)-b,_b).reshape(1)[0]) for txt,vec in enumerate(self.embeddings)]\n",
    "\n",
    "        return self.get_best_analogy(sim_list, string_b, return_cos_similarity)\n",
    "\n",
    "    def get_analogy(self, string_a, analogy_a, string_b, k_dim=0, return_cos_similarity=False):\n",
    "        \"\"\"\n",
    "        get analogous words using 3COSADD, PAIRDIRECTION, or 3COSMUL which make use of the cosine similarity of the embedding vectors.        \n",
    "        adapted from: https://www.aclweb.org/anthology/W14-1618\n",
    "\n",
    "        Args:\n",
    "            string_a, analogy_a - example of a string and its analogy\n",
    "            string_b - string whose analogy is to be determined\n",
    "            embeddings_dict - dictionary of embeddings. uses existing embeddings if was pretrained\n",
    "            return_cosine_similarity - whether or not output should include the analogy's cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            analogy of given string based on given example and determined using cosine similarity\n",
    "        \"\"\"\n",
    "        a, _a, b = (self.get_word_embedding(string).reshape(1,-1) for string in [string_a, analogy_a, string_b])\n",
    "        \n",
    "#         if self.compose_embeddings.comp_fn is None:\n",
    "#             return self._3_cos_add(a, _a, b, string_b, k_dim, return_cos_similarity)\n",
    "            \n",
    "        return self._3_cos_mul(a, _a, b, string_b, k_dim, return_cos_similarity) \n",
    "\n",
    "    \n",
    "# swe = \n",
    "# /text/analogies?a=wanacheza&b=atacheza[&type=sed] (default: sed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c4bb2c-1d1b-49f5-83e2-0e710afd4c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2a9fb5-c93e-48cf-bbaa-7a215de7dc3b",
   "metadata": {},
   "source": [
    "## Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5607a-3cf0-4288-91a3-9a42b7cc9907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
